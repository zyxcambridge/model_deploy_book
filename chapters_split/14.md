# 五、Algorithm Level (Sec.4)


大型语言模型（LLMs）在各种文本生成任务中已经取得了惊人的表现。它们通常包含一个解码阶段，该阶段按照与所有前导词的自回归关系逐个生成词元。在每个词元解码过程中，都需要反复将解码器权重加载到内存中。随着LLM参数规模变得庞大，解码过程变得严重受内存限制 导致硬件利用率低下，从而引发极长的延迟问题  Kim等人。这对于需要快速甚至实时响应的真实世界应用，如聊天机器人，尤为成问题。因此，迫切需要优化解码过程以提高这类应用中的性能。

本节专注于从算法角度讨论减少LLM推理成本的先前努力。具体而言，本节旨在从两个方向展开讨论：

• 在第4.1节中，针对解码每一个单独的词元（固定解码词元数量），如何使用最少的LLM参数。 • 在第4.2节中，对于LLM每一次的前向传播（使用固定数量的参数），如何解码最多数量的词元。

## Fast Decoding Algorithm (Sec.4.1)

#### 4.1. Minimum Parameter Used Per Token Decoded

有趣的是，Simoulin和Crabb已经表明，尽管语言模型往往具有大量的参数，但并非所有参数都是生成准确词汇所必需的。通过为每个输入词汇选择仅需的一部分参数进行加载，LLM（大型语言模型）的推理延迟可以减少，同时仍能保持解码词汇的准确性。在本节中，我们将从三个不同的角度探讨针对LLM的输入依赖型动态权重裁剪方案：4.1.1着眼于早期退出或动态地在层、深度、维度中选择权重；4.1.2介绍了一些方法，这些方法动态地检测LLM宽度维度中的稀疏性，剔除注意力头和MLP（多层感知机）列；4.1.3展示了混合专家（MoE）方法，该方法预先训练一个稀疏模型，并在运行时为不同输入选择正确的专家。

##### 4.1.1 Early Exiting

早期退出（或层跳过）是各种网络架构中一个被广泛研究的想法，特别是在编码器-only模型中Hou 等人对于解码器架构的早期退出需要在序列级别上保持一致性与质量，因为每个令牌依赖于前面的令牌，这是之前丰富的仅编码器早期退出文献中所缺乏的考虑。解码器包含结构相同的层。得益于这一特性，每一层的输出隐藏状态都可以用于传递到LM Head以获得下一个解码令牌的概率分布预测。Geva 等人 和 Simoulin 和 Crabbé 观察到，对于某些令牌，中间层的隐藏状态会饱和。换句话说，对某些令牌来说，在中途退出也能像完整模型运行那样，输出正确的顶级预测。这一观察为解码器早期退出方法的成功奠定了基础。

Elbayad 等人在高效机器翻译任务上进行了早期尝试，旨在解码器架构上使用早期退出。它提出了一种通用方法，如图12（b）所示，在前向传播过程中，每经过一层后，都有一个内部置信度函数，通常是固定的度量标准或具有少量层数的MLP，基于隐藏状态计算一个置信度分数，判断其在当前层是否可能达到饱和。该分数用于决定是否通过一些精心设计的标准来退出。然后，LM Head用于输出下一个令牌预测的概率分布。

由于后续工作的高度相似性，我们通过探讨为语言模型设计早期退出方案的关键挑战来扩展讨论，其中它们引入了不同的新颖技术。

##### 模型饱和信心的建模

CALM  Schuster 等人研究了三种不同的方法来输出退出的置信度分数：softmax响应，或softmax之后的前两个值之差；隐藏状态的饱和度，或当前层与最后一层的隐藏状态之间的余弦相似度；以及插入到每一层的线性分类器的输出。线性分类器通过简单地使用交叉熵损失来训练，目的是对输入隐藏状态时，分类器的输出与当前层退出时解码出的顶级令牌是否与完整模型解码出的顶级令牌匹配进行校准。实验表明，尽管线性分类器不总是最准确的预测器，但它在生成分数时达到了额外FLOPs开销与预测精度之间的最优平衡。基于CALM， Bae 等人观察到持续从浅层退出会导致异常长的序列长度。同时，每层的置信度计算注入了高开销，削弱了早期退出的好处。因此，它提议只提供两种早期退出的选择：要么从所谓的“浅模块”或一组浅层退出，要么一路走到全模型，即“深模块”，这大大减少了模型内部所需的分类器数量。这样的设计使其比CALM实现了更多的加速，在某些任务上达到了2倍。另一方面，ConsistentEE Zeng 等人 提出了一个不同的方法来预测何时退出。它使用了一个迭代训练的基于策略的网络，该网络带有每层输出分类器头。策略网络的目标是平衡效率（早期层接收奖励）和准确性（奖励函数中有一项是早期退出输出的交叉熵损失）的优化。

早期退出标准。Schuster等人在2022年的CALM工作中提出了一种分布无关的校准技术，该技术采用固定序列测试程序（家族错误率程序）来输出合适的阈值。这个阈值以指数方式递减，以便对序列中后面的令牌采取更积极的退出策略。另一方面，Bae等人在2023年的研究中观察到置信标准的模式类似于beta分布，并利用在线数据通过最大似然估计（MLE）更新beta分布模型，然后用这样的概率模型来指导其决策过程。Zeng等人在2023年则通过让策略网络直接输出退出决策来绕过这个问题。

隐藏状态传递。被跳过的层的隐藏状态可能带来技术挑战。如图12(b)所示，在"school"这个位置的令牌比前面的令牌更晚退出。然而，最后一层自注意力层并没有之前提前退出令牌的键值对。Elbayad等人在2020年和Schuster等人在2022年提出了“隐藏状态传递”技术。例如，存储在退出层l1的令牌"Max"的隐藏状态。当后来的令牌"school"到达更深的层l2时，从l1到l2之间的所有层都复制"Max"的隐藏状态，并基于这些复制的隐藏状态计算键值对。这基本上是用早期层的隐藏状态来近似深层数的隐藏状态。后来的工作，如Bae等人在2023年和Ding等人在2023年的研究发现，状态传递会导致性能下降。由于大型语言模型推理主要受内存加载的限制，计算相对而言是“免费”的。这两种方法提议直接即时重新计算后续的隐藏状态。Chen等人在2023b年提出并行运行完整的大型模型与早期退出流，以便高效地并行计算缺失的kv缓存。Din等人在2023年针对变压器架构使用线性网络跨层跳跃进行了系统研究，并表明可以添加线性层有效弥合直接复制和计算隐藏状态之间的性能差距，同时保持低内存和计算成本。SkipDecode Corro等人在2023年选择了一种激进的方法，优先考虑加速并放松性能保留的目标。利用观察到同一序列中后面出现的令牌平均需要较少的层数来解码正确令牌的现象，它完全绕过了状态传递的需要，强制最大使用的层数随位置加深而单调递减。此外，SkipDecode还引入了固定的退出点，以优化批处理的早期退出。

输出分类器训练。当从中间层退出时，中间的隐藏状态需要经过一个输出分类器头部，以输出下一个令牌概率分布的预测。输出分类器可以是共享的，如图12所示，也可以是每层独立的。这些分类器通常经过训练以便更好地适应早期退出模式。Elbayad等人提出使用所有层的交叉熵损失平均值作为分类器的训练损失。另一方面，Schuster等人采用加权平均，其中权重随着层数的增加而增加，给予更深的层次更多的贡献。Bae等人引入了一种动态知识蒸馏损失，该损失动态地为“浅层模块”分配来自“深层模块”的合适隐藏状态。Rotem等人和Ji等人都发现了在所有模型上使用相同损失进行联合训练时的“梯度冲突”问题：Rotem等人检测到语言模型中早期层与后期层之间的梯度冲突，而Ji等人则注意到提高语义意识的目标与改善早期退出决策目标之间的“正交梯度”。两种方法都提出了添加额外参数块和迭代训练来缓解这一问题。除了上述观点，Chen等人研究了在3D并行设置下高效运行大型语言模型（LLM）早期退出的系统级优化技术。

## Parameter Usage Reduction (Sec.4.2)

### (1) Early Exiting

### (2) Contextual Sparsity

早期退出技术旨在深度维度上选择参数，而一些技术也已提出以利用模型宽度维度上的动态稀疏性。《DejaVu》（刘等人c）对大规模语言模型（LLM）宽度维度上的动态稀疏性进行了全面研究。该论文揭示，上下文相关的稀疏性可以高达80%，这意味着大部分权重可以被舍弃而不影响模型的原始性能。然而，被选中的权重是动态的，针对不同的输入令牌各不相同。论文将此问题表述为一个近邻搜索问题，即对于来自前几层嵌入层的隐藏状态，如何找到与这些令牌最相似的注意力头和多层感知机（MLP）列。为了节省计算资源，论文提议在多头注意力（MHA）和前馈网络（FFN）的LLM前方训练一个小型的MLP网络作为“稀疏预测器”。通过仅使用一部分权重并减少内存I/O开销，DejaVu实现了超过2倍的LLM推理速度提升。

基于DejaVu的研究，PowerInfer（宋等人将上下文稀疏性发现应用到了跨异构设备（CPU和GPU）的LLM推理中。PowerInfer发现，很大一部分权重在输入无关的设置下被频繁使用和激活，因此存储在GPU内存中，而其他权重则存于CPU内存中。然后，为了特别找出给定输入令牌应使用的权重，它训练了一个比DejaVu更小的稀疏预测器。为了构建稀疏预测器，它初始化预测器具有动态结构，并进行迭代训练及调整。为了更好地在混合CPU和GPU环境下进行模型推理，它引入了一种新颖的内存放置方案并实现了一个基于向量的稀疏计算库。

同时，MatFormer（Devvrit等人）研究了在不同硬件能力的异构设备上部署LLM的问题。他们仅在占总权重60%的FFN上增加了动态结构。该模型经过专门训练，以便在推理时，根据目标硬件特性，在行维度上对MLP层进行采样，从而生成各种大小且性能合理的模型。为了丰富模型尺寸的选择，它采用了一种“混搭”方法，为不同层选择不同的设置，组合起来可提供更灵活的模型尺寸选择。

### (3) MoE

语言模型，尤其是基于变换器架构的模型，在训练数据集增大时展现出明显的幂律缩放特性（Kaplan 等人，Hoffmann 等人）。然而，尽管带来了显著的性能提升，大参数量使得模型的训练和推理效率低下。专家混合（Mixture of Experts, MoE）技术是一个被广泛研究的主题（Yukseke 等人），它有效地解耦了模型的参数数量与训练和推理所需的计算浮点运算次数（FLOPs），在特定条件下带来了巨大的效率提升。此外，MoE 已被证明能够无顾虑地扩大语言模型的规模并提高其性能，而不增加推理期间的计算量（Lepikhin 等人，Fedus 等人）。如图12(d)所示，专家网络被插入到变换器架构中以替换FFN层。同时，在多头注意力层和专家网络之间引入了一个门控函数，旨在为给定的输入令牌选择最合适的专家或专家组合。对于关于MoE缩放、泛化能力、路由算法、训练技术等的深入分析和讨论，我们推荐读者参考稀疏专家模型的综述（Fedus 等人）。虽然两者都依赖于输入令牌来决定稀疏结构，但我们特意将MoE和上下文稀疏技术分开，因为后者作用于预训练的密集型语言模型，并从密集神经网络中挖掘稀疏性，而前者从一开始就训练稀疏模型。最近，MoE技术已取得了重大成功。Sparse Mixer（Lee-Thorp 和 Ainslie）为BERT模型（Devlin 等人）在训练和推理上分别带来了89%和98%的速度提升。Du等人仅使用了49%的FLOPs，但性能上超过了GPT-3（Brown等人）。ST-MoE（Zoph 等人）将MoE应用到编码器-解码器模型中，甚至成为了许多推理和生成任务的最先进模型。ST-MoE在训练和推理中分别使用了比5400亿参数的PaLM（Chowdhery 等人）少20倍和40倍的FLOPs，但在性能上超越了它。Mixtral 8x7B（Jiang 等人）在推理时仅活跃使用130亿参数，但在各种评估基准上与Llama2-70B模型（Touvron 等人）表现相当。此外，针对MoE模型推理的优化也进行了多种尝试。Kossmann 等人 构建了一个高效的编译库RECOMPILE，为MoE模型引入了根据变化的推理批次大小动态重新编译和优化的功能。Rajbhandari 等人 将ZeRO分布式推理方法扩展到了MoE模型上。Jawahar 等人 在专家网络架构上进行了神经架构搜索（NAS）。Yi 等人 将大型MoE语言模型部署到了边缘设备上，围绕某些神经元在MoE模型中使用频率远高于其他神经元的发现进行优化部署。

### 4.1 .4 Roofline Model Analysis for Dynamic Parameter Reducing

最小参数每令牌解码方法同时减少了计算和内存访问的开销。从Roofline model的角度来看，这些方法导致每个操作的算术强度发生小幅度变化以及约束类型的改变。 对于早期退出或层跳过方法，整个Transformer层被跳过，从而成比例地减少了整体计算量、内存访问量和推理时间。换句话说，推理时间的减少与网络中跳过的层数成正比。然而，对于上下文稀疏性和专家混合这样的方法，算术强度在不同操作之间有所变化。因此，动态选择激活这些层会导致计算和内存访问的减少程度不一，进而对总体推理时间产生不同的影响。

## Maximizing Decoding Tokens (Sec.4.2)

减少大规模语言模型（LLM）推理延迟的另一个角度是放松模型受自回归解码的限制，使得一次模型前向传播能解码出多个令牌。我们从两个方面来探讨这一目标的实现：

4.2.1 **推测性解码方法**：这一方法引入了一个计算效率高的草稿模型，用于为接下来的几个令牌位置提出候选令牌，而LLM则用于评估草稿模型提出的这些草稿令牌，而不是直接生成下一个令牌。这样，草稿模型承担了快速预览和提案的角色，而LLM则专注于质量验证，从而可能加速整个解码过程。

4.2.2 **多令牌并行解码**：另一方面，这一类工作致力于使LLM能够直接从单次前向传播中解码出多个令牌。这类方法通常涉及到对模型架构或解码策略的根本性修改，以支持并行处理和输出多个令牌，从而显著减少解码序列所需的迭代次数和时间。

由于某些方法结合了上述两方面的优点，处于两者之间，为了命名上的清晰，我们人为地做出以下区分：这里提到的**推测性解码方法**特指那些草稿模型同样采用变换器架构的情况。这样的划分有助于更精确地区分不同策略的核心差异及其技术实现路径。

### (1) Speculative Decoding

由于大规模语言模型（LLMs）在推理过程中面临严峻的内存加载挑战及自回归特性，导致效率不高。然而，有研究表明（Kim等人e），只要对小型模型生成序列中的某些关键令牌进行修正，这些远小于LLMs的模型也能够解码出与LLM一样正确的序列。如图13（a）所示，当要求小型模型进行推测并输出一系列草稿令牌时，模型权重的内存加载不再是大问题，从而在硬件计算单元上实现了更高的利用率。为了确保小型模型生成文本的质量，LLM可以“定期”评估并修正来自小型模型草稿中的令牌。尽管大型模型有时需要评估错误的草稿令牌，这可能比LLM自回归解码带来更多的浮点运算（FLOPs）消耗，但权重的内存加载在令牌维度上实现了并行化，从而极大地减少了内存I/O开销。鉴于LLM推理主要受内存瓶颈限制，推测性解码有望大幅度降低LLM推理延迟。

在探索这个想法的早期阶段，出现了两条并行的道路。Kim等人提出了一种方法，让小模型进行推测并生成草稿token，直到解码出的token的置信度低于某个阈值。然后，小模型“回退”到大模型来评估已生成的草稿token，并交回给小模型处理。其中一些token会被拒绝，因此大模型要求小模型“撤销”这些错误的token并继续推测。论文中的所有解码都是“贪婪”的。该论文表明，大模型和小模型的组合能够生成与原始大模型自回归生成文本质量相当的文本。

然而，Leviathan等人和Chen等人基于小模型推测范式，指出了一个在LLM拒绝小模型预测的位置进行重采样的技术，这被证明能够使大模型和小模型的预测处于与大模型自回归生成相同的概率分布中。随后的技术通常遵循推测、评估和重采样的范式，以便在保持LLM自回归解码质量的同时实现加速。

在探索这个想法的早期阶段，出现了两条并行的道路。Kim等人提出了一种方法，让小模型进行推测并生成草稿token，直到解码出的token的置信度低于某个阈值。然后，小模型“回退”到大模型来评估已生成的草稿token，并交回给小模型处理。其中一些token会被拒绝，因此大模型要求小模型“撤销”这些错误的token并继续推测。论文中的所有解码都是“贪婪”的。该论文表明，大模型和小模型的组合能够生成与原始大模型自回归生成文本质量相当的文本。

然而，Leviathan等人和Chen等人基于小模型推测范式，指出了一个在LLM拒绝小模型预测的位置进行重采样的技术，这被证明能够使大模型和小模型的预测处于与大模型自回归生成相同的概率分布中。随后的技术通常遵循推测、评估和重采样的范式，以便在保持LLM自回归解码质量的同时实现加速。

构建草稿令牌树

由于LLM按自回归顺序生成，每个令牌都依赖于之前生成的所有令牌，且小型模型草稿中接受的令牌长度通常是适度且有限的。对更远未来的令牌进行推测难度呈指数级增加。例如，如果要求小型模型输出长度为m的草稿序列，而LLM仅接受其中的n个令牌（n < m），那么额外的(m - n)个令牌会自动被丢弃。因此，推测性解码的速度提升比率较为有限，因为每次LLM前向传播只能解码有限数量的令牌。

为了提高推测性解码的速度提升，有两种改进方式。首先，孙等人（Sun et al., 2023b）、苗等人（Miao et al., 2023b）以及许等人（Xu et al., 2023a）均提议从批量大小方向增强草稿，或者让小型模型并行地为LLM采样多个可能的草稿序列进行评估。具体而言，孙等人（2023b）提出了一种方法及其理论保障，使LLM能够批量验证从小型模型草稿中的多个样本重新采样，从而保持LLM分布不变，且不损失生成质量。该论文首先将推测性解码与离散最优传输这一更广泛的问题联系起来。小型模型被要求使用top-k采样来生成多个草稿序列。基于离散最优传输的性质，寻找最佳的评估与重采样方法转化为寻找最优的传输路径问题。

另一方面，除了保持草稿树的推测性解码一致性之外，Miao等人构建令牌树的方法并非基于小型草稿模型的顶级预测，而是基于多个经过多样性训练的小型草稿模型。每个模型并行运行，输出多样化但强大的草稿序列。该论文提出了一种新颖的草稿令牌树构建算法，通过预定义的扩展和合并策略，根据多样化的草稿序列建立候选令牌树。随后，大型模型被要求并行验证构建的树结构，采用精心设计的树注意力机制以最大化键值缓存的复用，并维持基于树的因果遮罩。Xu等人创新性地将推测性解码的好处应用于边缘设备。该论文为边缘设备构建了一个LLM服务引擎，其中较小的草稿LLM持续驻留在内存中，而较大的、健壮的LLM则偶尔加载到内存中进行验证。为了提高大型LLM的接纳率，它也利用top-k令牌构建树结构。为了适应边缘硬件特性，实现了一种基于树的并行验证解码器，配备了遮蔽功能，并定制了大小LLM计算管道以避免内存争用，从而优化了资源使用并提升了推理效率。

知识蒸馏与自我推测性解码另一种提升接纳率的方法是增强小型草稿模型与LLM生成分布的一致性，这可以通过在大型模型生成的语料上利用知识蒸馏对小型模型进行微调来实现。周等人建立了接纳率与小型模型与LLM之间自然分歧之间的数学联系：最小化这种分歧即最大化接纳率。该论文还研究了一系列不同的知识蒸馏损失函数，并表明添加知识蒸馏能带来持续10%-45%的延迟加速改进。然而，总体而言，论文发现最优的知识蒸馏损失选择因模型而异，应作为超参数进行调整。刘等人同样显示知识蒸馏能促进小型模型的训练。此外，该论文还将推测性解码引入云在线学习场景。LLM推理受内存瓶颈限制，这意味着总是存在计算资源过剩。这些额外的计算能力可以用来在服务器上持续训练草稿模型，带来两个好处：持续的知识蒸馏训练提升其接纳率，从而减少LLM推理延迟；2)服务输入领域不断变化，持续训练有助于草稿模型在不同领域保持强大性能。张等人通过从大型模型本身有选择地抽样出一个小型草稿模型来避免存储独立的草稿模型。部署前，论文利用贝叶斯优化方法在预训练的大型模型内部跳过中间层来搜索一个草稿模型。此外，针对从大型模型抽样的草稿模型解码，提出了适应性阈值选择技术。

### (2) Parallel Decoding

另一方面，大量工作已被提出以使大型模型能够直接进行并行解码，无需小型变换器模型的帮助。同时预测多个未来令牌 许多研究正在探索如何直接从大型语言模型的一次前向传播中实现对多个令牌的直接预测。Stern等人开创性地设计了在最后一个隐藏状态输出与语言建模头输入之间插入一个线性投影层的方法，使得可以仅基于当前令牌的最后一个隐藏状态作为输入来投射多个未来的令牌。随后由LLM进行评估，以决定是否接受或拒绝这些投射出的令牌。该提议的技术主要针对具有解码器结构的序列到序列模型。最近，Cai等人将先前的工作扩展到了仅有解码器的语言模型，如图13（b）所示。除了最后一层的投影外，为了进一步提高解码的接受率，论文建议增加一个基于树的解码结构及相应的注意力掩码设计，以便大型模型同时评估多个草案。同时，Monea等人提出在输入序列的末尾添加几个虚拟令牌，这些在工作中被称为“前瞻嵌入”。在每一层的前向传播过程中，以前的提示令牌和已解码令牌的信息可以用来并行解码几个连续的未来令牌。为了实现这一设计，该工作训练了一个单独的嵌入层，专门服务于这些前瞻嵌入。Li等人也旨在使用LLM评估进行并行解码。与之前的工作相似，它也添加了一个轻量级结构FeatExtrapolator。不同的是，该结构同时将前一个令牌的最后一层隐藏状态和实际解码的令牌嵌入作为输入，并输出下一层的隐藏状态预测。使用LLM的LM头，抽取几个令牌，然后用这些令牌构建一个解码树，供LLM并行评估。

除了直接利用LLM输出后续若干个令牌外，一些工作利用自然语言中频繁出现的n-grams来实现在大型模型的一次前向传播中生成多个未来令牌。LLMA（Yang等人）首先观察到生成任务往往要求LLM重复之前上下文中出现过的令牌。基于这一发现，该论文着手利用解码出的令牌和提示与一组参考文档进行前缀匹配，以便一旦发生重复，可直接将重复的令牌复制到当前位置。然后，LLM会评估从先前上下文中找到的这些候选令牌，决定是否使用它们。He等人进一步扩展了LLMA，提议首先基于LLM预训练或微调的数据集和语料库构建一个常用短语数据库。解码过程中，将之前的上下文提示或令牌作为查询，用于检索构建的数据库。检索到的候选短语被组织成前缀树结构或字典树（Trie），使得LLM能高效地进行评估。Lan等人同样沿用了检索方法加速推理过程，不同之处在于其在LLM末尾增加了一个额外的注意力层，利用当前令牌隐藏状态表示的当前上下文作为查询，关注从参考文档中检索到的相关短语，并基于注意力分数选择排名靠前的短语。这些方法共同展示了如何通过集成外部知识和数据结构优化，有效提升语言模型的生成效率与质量。

语言中的层次结构确实存在。在撰写长篇文章时，通常的做法是先概述文章的大纲，以要点的形式列出。然后，针对每个要点，可以扩展论据来充分表达该要点的全部意图。根据观察，不同要点之间的语义相对独立，因此提出了一些方法来并行处理不同要点的生成过程。

“思想骨架”（Skeleton-of-Thoughts，宁等人）提出首先让大型语言模型（LLM）生成文章的简洁要点，然后将这些要点在批量轴上收集起来，并再次作为提示输入到LLM中，要求模型并行地为每个要点展开论据。这种方法实现的大约2倍速度提升，但缺点是不易泛化到所有文本生成任务上。

最近，“APAR”沿着这一方向进行了扩展。该论文添加了特定的软性标记（soft tokens），在生成过程中明确告知LLM层次结构信息。LLM进一步接受了指令微调，以整合这些新增的特殊标记，并利用Medusa技术（蔡等人）增强了生成过程，实现了带有层次结构的文本生成速度4倍的提升。

Song等人率先研究了使用可并行化方法近似全连接网络或卷积神经网络（CNNs）迭代和顺序推理的结果。尽管看似不可行，该论文发现神经网络能够容忍数值近似误差，且神经网络学习的数据模式在一定程度上揭示了并行结构，这使得在某些场景下对神经网络的顺序推理进行并行化成为可能。雅可比（Jacobi）和高斯-赛德尔（Gaussian-Seidel）迭代算法此前被提出用于求解非线性方程组（Ortega和Rheinboldt ），并显示出能有效地并行化神经网络的顺序推理。

Santilli等人将雅可比和高斯-赛德尔算法扩展到机器翻译任务中的自回归解码并行化。具体而言，这项工作建立在先前的非自回归变换器架构之上，以利用GS-Jacobi算法增强并行解码。并行解码过程在解码文本中发现\[EOS\]（结束）令牌时停止。

同时，前瞻解码（Lookahead Decoding，Fu等人扩展了这种方法，以并行生成LLM的后续令牌。除使用基本的雅可比迭代算法外，它还通过基于检索的算法重用先前见过的n-grams来提升速度。此外，通过向原始LLM模型引入精心设计的注意力遮罩，它并行化了前瞻步骤和LLM验证步骤，进一步提高了解码效率。这些方法代表了对传统序列生成模型并行化处理的创新尝试，旨在解决长序列生成中的延迟问题，提升整体计算效率。

针对那些需要自回归解码序列到序列模型的任务，非自回归变换器（Non-Autoregressive Transformers, NAT）被提出以迭代地同时解码所有输出令牌，如图13（d）所示。NAT已被相对广泛地研究，并且我们引导读者参考专门针对NAT模型的综述论文Xiao等人,该文深入地回顾和分析了这一主题。粗略地说，文本解码速度的提升来自于使解码器的前向传播一次性处理多个令牌。输入序列首先被送入编码器，后者输出提取输入语义的隐藏状态。然后，编码器的输出隐藏状态被用作解码器传递的条件。为了加速文本生成，解码器端放松了自回归约束，以充满哑元标记\[pad\]的序列作为输入开始并行解码过程。在每次迭代中，基于编码器输出隐藏状态设置的条件，一些令牌可以被自信地预测并解除遮蔽。序列混合了解码的未遮蔽令牌和剩余的遮蔽令牌再次被送入解码器，直到每个令牌都被解码。送入解码器的序列长度，或称为生育率（fertility），通常要么在编码器内部作为特殊\[CLS\]标记学习，要么通过一个专门的生育率预测器在编码器和解码器之间学习。最近，Savinov等人将解码器视为一个扩散模型，并训练其根据给定的条件去噪初始的噪声序列。然而，由于需要使用编码器的隐藏状态作为并行解码的条件，NAT方法在直接扩展到仅解码器架构时面临固有的困难。

## Serving技术

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/bdaa7365-2dbd-4a41-9bda-1dcd7280ec69.png)

O1:

低成本，算力增加；

### 投机采样（Speculative Sampling） 详细解释

投机采样（Speculative Sampling）是一种用于加速 **自回归模型（Autoregressive Models）** 推理的技术，特别适用于生成任务（如文本生成、图像生成等）。自回归模型的推理过程通常是逐 token 生成的，即每次生成一个 token 并将其作为输入用于生成下一个 token。这种逐 token 生成的方式导致推理速度较慢，尤其是在生成长序列时。

投机采样的核心思想是通过 **并行生成和验证** 来加速推理过程。它利用一个较小的“草稿模型”（Draft Model）快速生成多个候选 token，然后使用主模型（Large Model）并行验证这些候选 token 的正确性。通过这种方式，投机采样可以显著减少推理时间，同时保持生成质量。

---

### 1. 自回归模型的推理瓶颈

在传统的自回归模型中，推理过程是逐 token 生成的：

1.  输入 prompt \(x\)，生成第一个 token \(y\_1\)。
    
2.  将 \(y\_1\) 加入输入，生成第二个 token \(y\_2\)。
    
3.  重复上述过程，直到生成完整的序列。
    

这种逐 token 生成的方式导致：

*   **高延迟**：每个 token 的生成都需要等待前一个 token 的生成结果。
    
*   **低硬件利用率**：GPU/NPU 等硬件设备的并行计算能力未被充分利用。
    

---

### 2. 投机采样的核心思想

投机采样通过以下步骤加速推理：

1.  **草稿模型生成候选序列**：
    
    *   使用一个较小的草稿模型（Draft Model）快速生成多个候选 token（如 \(y\_1, y\_2, \dots, y\_k\)）。
        
    *   草稿模型的计算量远小于主模型，因此可以快速生成候选序列。
        
2.  **主模型并行验证候选序列**：
    
    *   使用主模型（Large Model）并行验证草稿模型生成的候选 token。
        
    *   主模型会计算每个候选 token 的概率分布，并与草稿模型的输出进行比较。
        
3.  **接受或拒绝候选 token**：
    
    *   如果主模型验证通过（即候选 token 的概率高于某个阈值），则接受该 token。
        
    *   如果验证失败，则丢弃后续候选 token，并使用主模型重新生成正确的 token。
        
4.  **重复过程**：
    
    *   将已接受的 token 加入输入，重复上述过程，直到生成完整的序列。
        

---

### 3. 投机采样的具体步骤

以下是一个具体的投机采样流程：

#### 步骤 1：草稿模型生成候选序列

*   输入 prompt \(x\)，草稿模型生成 \(k\) 个候选 token：\(y\_1, y\_2, \dots, y\_k\)。
    
*   草稿模型的计算量较小，因此可以快速生成候选序列。
    

#### 步骤 2：主模型并行验证候选序列

*   主模型接收 prompt \(x\) 和候选序列 \(y\_1, y\_2, \dots, y\_k\)，并行计算每个位置的条件概率分布： \\[ P(y\_i | x, y\_1, y\_2, \dots, y\_{i-1}) \\]
    
*   主模型会输出每个候选 token 的概率分布。
    

#### 步骤 3：接受或拒绝候选 token

*   对于每个候选 token \(y\_i\)，比较主模型和草稿模型的输出：
    
    *   如果主模型的概率分布与草稿模型的输出一致（或差异小于某个阈值），则接受该 token。
        
    *   如果不一致，则丢弃后续候选 token，并使用主模型重新生成正确的 token。
        

#### 步骤 4：更新输入并重复

*   将已接受的 token 加入输入，重复上述过程，直到生成完整的序列。
    

---

### 4. 投机采样的优点

1.  **加速推理**：
    
    *   通过并行生成和验证，减少逐 token 生成的延迟。
        
    *   草稿模型的快速生成和主模型的并行验证显著提高推理速度。
        
2.  **保持生成质量**：
    
    *   主模型的验证确保生成的 token 符合高质量标准。
        
    *   即使草稿模型生成错误，主模型也能及时纠正。
        
3.  **资源效率**：
    
    *   草稿模型的计算量较小，适合在资源受限的设备上运行。
        
    *   主模型的并行验证充分利用硬件设备的计算能力。
        

---

### 5. 投机采样的挑战与优化

#### 5.1 草稿模型设计

*   **挑战**：草稿模型需要快速且准确，否则会导致大量候选 token 被拒绝。
    
*   **优化**：
    
    *   使用轻量级模型（如蒸馏模型）作为草稿模型。
        
    *   通过训练使草稿模型的输出分布接近主模型。
        

#### 5.2 验证策略

*   **挑战**：如何高效地验证候选 token，避免过多的计算开销。
    
*   **优化**：
    
    *   使用概率阈值判断是否接受候选 token。
        
    *   设计高效的并行验证算法。
        

#### 5.3 错误恢复

*   **挑战**：当候选 token 被拒绝时，如何快速恢复生成过程。
    
*   **优化**：
    
    *   使用缓存机制保存已验证的 token。
        
    *   设计快速回退策略，避免重新生成整个序列。
        

---

### 6. 投机采样的应用场景

1.  **文本生成**：
    
    *   加速 GPT 系列模型的文本生成过程。
        
    *   适用于对话系统、内容创作等实时应用。
        
2.  **图像生成**：
    
    *   加速自回归图像生成模型（如 PixelCNN）的推理过程。
        
3.  **语音合成**：
    
    *   加速自回归语音合成模型（如 WaveNet）的推理过程。
        

---

### 7. 与其他技术的结合

#### 7.1 投机采样 + 量化

*   **方法**：对草稿模型和主模型进行量化，进一步减少计算量和内存占用。
    
*   **优点**：显著提高推理速度，适合端侧部署。
    

#### 7.2 投机采样 + 剪枝

*   **方法**：对草稿模型进行剪枝，减少其计算量。
    
*   **优点**：进一步提高草稿模型的生成速度。
    

#### 7.3 投机采样 + MoE

*   **方法**：使用 MoE 技术优化主模型的计算效率。
    
*   **优点**：在保持生成质量的同时，进一步加速推理。
    

---

### 8. 总结

投机采样是一种通过并行生成和验证加速自回归模型推理的技术。它利用草稿模型快速生成候选序列，并通过主模型并行验证候选 token 的正确性。通过这种方式，投机采样可以显著减少推理时间，同时保持生成质量。在实际应用中，投机采样可以与其他优化技术（如量化、剪枝、MoE 等）结合，进一步优化推理效率。

## 减少计算量

### D1.连接结构优化

### D2.卷积通道配置

### D3.通道间分组

### D4.维度间分解

### D5.通道间变换（激活通道间)