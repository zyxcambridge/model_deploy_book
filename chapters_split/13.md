# 四、Hardware Optimization (Sec.6)


动态剪枝与量化 智能模型压缩技术,平衡性能与效率 硬件感知优化 针对特定硬件(如GPU、TPU或边缘设备)的深度优化 实时推理优化 如缓存中间结果、减少冗余计算等

设计硬件以高效支持LLMs（大型语言模型）的推理是一项艰巨任务，这主要是因为不同的推理阶段和工作负载条件下算术强度变化较大。具体来说，预填充阶段通常利用GEMM（通用矩阵乘法）运算符来处理批量化后的令牌，表现出较高的算术强度。相反，解码阶段则是逐个计算输出令牌，这就需要使用GEMV（向量与矩阵相乘）运算符或较为精简的GEMM运算符来处理注意力层和前馈网络（FFN）层。这些操作的特点是算术强度低。

深度神经网络模型在智能任务中取得了令人瞩目的成就，然而，这些优异的表现往往需要庞大的计算资源的支持。因此，神经网络模型的发展不仅依赖算法的不断革新，也离不开背后硬件平台的推动。例如，战胜李世石的 AlphaGo405| 背后使用了1920块 CPU 和280块 GPU进行计算。GPU 最初是作为辅助 CPU 进行图形计算的硬件。近十年来，由于其高度并行的特性与 AI 任务的天然契合，GPU 已经成为目前运行深度学习应用最流行的硬件平台。为了更加高效地运行深度学习任务，GPU 制造厂商 NVIDIA 专门在其 GPU上增加了针对深度学习任务的专用处理单元，如张量核等。

然而，如图 9.1所示，随着软件算法和硬件设计的不断发展，新的需求和挑战也随之而来，对软硬件的性能和可拓展性提出了更高的要求。硬件方面，现有的硬件逐渐难以处理日渐增长的数据和需求；软件算法方面，模型不断变大，任务数量逐渐增加，并且任务负载可能存在时变。面对这些全新的挑战，一方面，实际硬件计算平台中，算力、存储、带宽、功耗、成本都会受到限制，为了满足各种算法应用和硬件部署场景的极致性能优化需求，需要针对具体约束和实际应用需求进行软硬件协同设计，以期又快又好地实现符合设计指标的软硬件系统。另一方面，当前应用中需求的大模型、多任务和动态负载特性要求我们支持更大的计算量及动态的应用负载。因此，需要设计计算资源虚拟化系统，将实际物理资源抽象成虚拟化硬件资源池，在不同任务负载中动态分配，进一步提高软硬件的可拓展性和资源利用率。

### 硬件加速器设计和软硬件协同优化

9.2.1 从 CPU 到硬件加速器

神经网络模型的能力提升通常伴随着参数量的增加，这也导致所需的计算量和存储量的增加，给神经网络模型的高效部署带来了挑战。以一个典型的用于图像分类任务的卷积神经网络模型 VGG-16 为例，当输入图片像素为 224×224时，需要约390亿次浮点计算（单位为 FLOPs）。

如果需要处理更高分辨率的图片，那么所需的计算量将更大。

目前，新的神经网络模型算法仍然在不断增加模型大小和复杂度，带来了参数量和计算量的增大。因此，为了保证计算的延时和能耗在可接受的范围内，高效地部署神经网络模型，需要一个合适的硬件平台。当前的桌面级通用处理器的峰值性能约为 10~100 GFLOPS，即每秒

100亿到 1000亿次浮点计算。如果使用该处理器计算上述卷积神经网络模型，则完成一次推理大约需要几百毫秒。如果想要处理更高分辦率的图像，则需要更大的计算量，延时会更长。在对延时较为敏感的场景中，例如自动驾驶，需要及时对目标进行检测和识别，通常需要在毫秒级别完成计算，CPU 很难达到这样的算力。另一方面，如果是端侧的嵌人式 CPU，则性能通常只有高性能 CPU 的十分之一甚至百分之一，更难满足神经网络模型的实时处理需求。

相比于 CPU,GPU 具有高并行度的计算架构，峰值性能可达 10 TFLOPS。深度神经网络模型又具有计算流固定、计算量大、数据量大和高度可并行的特点，因此，GPU 可以很好地对神经网络模型的计算进行加速，并成了目前主流的神经网络模型计算平台。主流的深度学习框架，如PyTorcN0，TensoPlow o 等，都提供了使用GPU的接口。NWIDLA 也提供了深度学习计算的优化库 CuDNNAT，以加速神经网络模型在 GPU上的计算。然而，GPU 的功耗一般较高，以目前的项级桌面级 GPU RTX3030为例，其热设计功耗为350W。即使是移动端的谈人式GPU、如NVIDIA TX2 GIPU，其功耗也在 10W 左右，无法滿足猫侧物酸附（Intemetamhines.ToT）对低功耗（1uY~1N）的飛求。

除了 CPU 和 GPU 这类通用处理器，学术界和工业界已经研究和开发了许多基于现场可编程门阵列（Filed Programmable Gate Array, FPGA）或专用集成电路（Application SpecificIntegrated Circuit,ASIC）的专用AI 加速器。相比于通用处理器，专用加速器可以针对 AI任

务的数据流和控制流特点，设计特定的硬件架构，去除不必要的开销，以实现高并行度、高吞吐率、高能效的AI 计算，进而实现神经网络模型的高效部署。基于 FPGA 的 AI 加速器具有灵活的可编程性和更短的开发周期，功耗适中；基于 ASIC的 AI 加速器可以最大限度地实现高性能和低功耗的要求，代价是 ASIC 开发需要大量的人力和物力成本，时间周期也较长。

根据相关文献对基于 FPGA 和 ASIC 的AI加速器设计方案的总结，AI 加速器的设计方案可以大致分为两种：第一种是将某种神经网络模型直接“硬化”，即专门设计与该网络模型相对应的硬件架构；第二种是引入指令集和编译器，设计更通用的基于指令的AI加速器，将不同模型的推理过程编译为不同的指令序列，再在AI 处理器上执行。

第一种方案通常应用在基于 FPGA 的AI加速器中，原因在于ASIC 的开发成本和周期代价高，很难对持续发展的神经网络模型进行支持。由于 FPGA 具备灵活可编程的特点，fpga-ConvNet/409、DeepBurning/410| 等方案基于寄存器传输级（Register Transter Level,RTL）模板或高层次综合（High-Level Synthesis,HLS）实现神经网络模型到硬件结构的映射。神经网络模型具有层状的图结构，因此通常会将神经网络模型的每层或一部分子图映射成一个计算模块，完整的神经网络模型则由多个计算模块拼接而成。

第二种方案则面向神经网络模型的计算特点设计指令集（Instruction Set Architecture, ISA），使用指令来控制硬件的计算和访存。例如，5.10 节介绍的 Ange-Bye123） 和谷歌的TPUP使用的都是基于指令集的定制 AI 加速器设计方案。该方案为了生成对应的指令序列，需要引入软件编译器，实现神经网络模型到指令序列的转化。将神经网络模型的部署过程拆分成基于指令集加速器和软件编译器两阶段，硬件和神经网络算法可以较好地解耦，适用于 FPGA 和 ASIC实现。该方案的主要优势是可以使用相同的硬件架构在运行时支持多种神经网络模型，无须重新烧写 FPGA 或重新设计 ASIC的硬件架构。同时，软件编译器可以采用一些针对图或算子的优化策略来提高吞吐率和计算效率。由于软件编译器的开发代价远低于修改硬件结构，其可以快速地支持不同的神经网络模型。除了上述工作，目前主流的专用 AI 加速器还有中科院的DianNao、赛灵思的 DPU等。

9.2.2 AI 加速器中的软硬件协同优化

神经网络算法和硬件架构设计在整个部署流程中相互影响，因此，软硬件协同设计对神经网络模型应用的高效部薯至关重要。AI 系统的软硬件协同设计如图9.2所示，其中工作负载、峰值性能、计算效率这三点会影响神经网络模型的高效部署。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/ed30c3f6-9d17-4eb2-9bf9-98ca223a894e.png)

图 9.2 AI 系统的软硬件协同设计

工作负载代表神经网络模型的计算量和存储量，会影响AI 加速器的硬件设计。为了降低神经网络模型的工作负载，一种方式是采用变换域快速算法实现 2D 卷积，例如 FFT 4121 和

Winograd4a算法，这会改变乘法和加法的比例和访存模式。例如，文献［414］使用了 Winograd算法计算卷积，并且支持跨层调度。实验结果表明，该设计可以在相同 FPGA 平台上提供最高

7倍的加速。通过对模型进行剪枝，可以显著降低模型的工作负载。此外，一些神经网络模型可能具有稀疏性，可以利用其稀疏性，将稠密操作转换为稀疏操作，从而大帽降低计算量。然而、稀疏数据需要与稠密数据不同的存储格式，并且会影响整个计算流程的数据流。从算法的角度考虑，虽然存储量和计算量较小的轻量级模型便于高效部署，但这通常意味着泛化能力和算法准确率的降低。因此，需要结合硬件架构，合理地选择神经网络模型的架构和参数量、并且设计合适的快速算法和稀疏方案，从而保证部署的准确性和高效性。

峰值性能代表着 AI 系统的算力，通常与计算单元的数量和时钟频率成正比。峰值性能的提高通常伴随着功耗和成本的提高。一种降低成本的可行方案是简化计算时的操作，例如使用占用更少比特位的整型数据替代浮点计算。由于神经网络模型自身具有一定的鲁棒性，即使用

16 位或8位的定点数水替换原本的32 位深点计算，也可以几乎不引起算法准确度的损失。同时，可以针对神经网络模型不同的层使用不同的数据位宽，对不敏感的数据使用更低位宽的数据格式。峰值性能和数据表示精度之间的权衡会同时影响算法和硬件设计，因此需要软硬件协同优化。

计算效率代表硬件单元的利用率，优秀的存储架构设计可以计算单元提供足够的数据，保证计算单元一直在工作，不因为缺少数据导致停顿和计算单元利用率的降低。因此，需要合理设计片上和片外存储的架构、容量和连接方式。片上的高速缓存应该充分利用数据的局部性，尽可能增加数据的重用，减少片外访存的次数；当访问片外存储时，应该尽可能增加单次访存的大小，从而充分利用带宽。总数据量相同的情况下，单次大数据量的访问要比多次小数据量的访问更高效。当计算一些轻量级的网络模型时，片外访存常常会成瓶颈。特别是当多个核同时工作且共享同一块片外存储器时，带宽抢占会导致带宽瓶颈问题更加突出。另一个解决方案是增加片外存储的带宽，提升计算效率，但这也会导致功耗和成本的增加，需要设计人员在成本与性能之间进行折中。为了实现 AI 加速器的软硬件协同优化，通常需要事先确定模型负载在 AI 加速器上运行的效率瓶颈。首先，需要确定瓶颈是计算还是访存，只有针对瓶颈进行优化，才能最大限度地提升性能。当计算成为瓶颈时，访存单元有所空闲，应该通过降低模型计算量或增加计算并行度提高性能；当访存成为瓶颈时，计算单元有所空闲，此时应该通过降低模型参数量或者增加访存带宽提高性能。只有当计算和访存同时成为瓶颈，即计算单元和访存单元利用率均达到峰值，AI 加速器才发挥其全部实力。

Roofine 模型是识别性能瓶颈的常用分析手段之一4日。通过计算神经网络模型的总计算量和总内存访问量，得出模型的平均计算强度，然后，利用特定硬件的Roofine 模型确定该模型在该硬件上的理论性能上限。

## 1. Spatial Architecture (Sec.6.1)

LLM的解码过程涉及根据先前生成的单词逐个预测单词。然而，这一过程在长序列生成任务中可能成本高昂。这是因为模型在生成每个令牌时需要访问大量的权重和键值（KV）缓存，导致算术强度低。为了解决这个问题，已经开发了几种解决方案。其中一种解决方案是实现“空间架构”。与传统计算机架构不同，空间架构采用了计算的不同方法。它不是将计算过程折返成处理单元（PE）和主内存之间的多次交互，而是将计算分布在多个PE上。这种设计允许利用并行性，因为每个PE同时执行计算的一部分。此外，中间数据在PE之间流动，避免了每次都要写回DRAM的情况。

在空间架构中，每个处理单元（PE）负责计算的一个特定部分。为了促进高效通信，数据通常在相邻的PE之间移动。这不仅提升了性能，还实现了资源的高效利用。在这样的空间布局中，每个PE都有直接访问内存的权限。这使得多个处理单元能够同时访问内存，提高了信息进出内存的整体速度，从而增强了内存带宽和整体的大规模语言模型（LLM）推理性能。如图20所示，随着总内存带宽的增加，解码阶段线性层的性能可以显著提升。

以Groq为例，他们利用大规模处理单元（LPU） Abts等人构建了一个针对LLM推理的空间系统。该系统在Llama-2-70b模型上实现了每用户每秒超过300个标记的卓越速度 。另一个例子是Graphcore的智能处理单元（IPU），这也是另一种类型的空间架构，能高效执行大规模语言模型。

## 2. PIM and NMC (Sec.6.2)

关于PIM（Processing-in-Memory，存内计算）和NMC（Near Memory Computing，近存计算）在章节6.2中的中文解释，以下是相关信息的总结：

### PIM（存内计算）

1.  **定义与背景**：
    
    *   存内计算是一种将计算单元放置在主存（如DRAM）内部的技术，通过将计算单元和内存单元物理耦合在一起，减少数据在存储和处理器之间的传输，从而提高性能和能效比。
        
    *   该概念最初被称为存内处理（Processing In Memory, PIM），但为了避免与近存计算混淆，现在通常被称为存内计算。
        
2.  **技术实现**：
    
    *   **ReRAM架构**：ReRAM（Resistive Random Access Memory，电阻式随机存取存储器）是常用的PIM架构基础，其通过电流模式或电压模式进行计算。
        
    *   **电流模式**：通过流经单个ReRAM单元的电流来表示输入特征与权重的乘积，最终通过电流模数转换器（ADC）转换为数字值。
        
    *   **电压模式**：通过位线电容的放电速率与输入向量和权重矩阵的点积成正比，产生的位线电压通过ADC转换为数字值。
        
3.  **优势**：
    
    *   **带宽提升**：某些存算一体芯片能够提供TB/s级别的内存带宽，远超传统DRAM的几十GB/s。
        
    *   **功耗降低**：通过减少数据搬运，显著降低整个系统的总功耗。
        
    *   **响应速度提升**：减少数据搬运的延迟，使得实时数据分析和响应成为可能。
        
    *   **计算密集型任务的加速**：在深度学习等领域，通过在存储器附近执行矩阵乘法、卷积等运算，显著加速训练和推理过程。
        

### NMC（近存计算）

1.  **定义与背景**：
    
    *   近存计算是在靠近内存的逻辑中执行计算，通过将计算逻辑放置在靠近内存的位置，充分利用内存内部可用的巨大内存带宽。
        
    *   该架构通常使用高带宽电路集成技术（如2.5D和3D集成）来优化内存带宽和能效。
        
2.  **技术实现**：
    
    *   **3D堆叠内存**：如Micron的Hybrid Memory Cube (HMC)，在多DRAM层的堆叠下面合入了一个逻辑层，实现自定义逻辑。
        
    *   **硅中介层和硅通孔**：2.5D集成电路使用硅中介层或有机中介层连接内存芯片和逻辑芯片，3D集成电路使用硅通孔和微凸块技术来堆叠多个DRAM层。
        
3.  **优势**：
    
    *   **内存带宽提升**：通过高带宽电路集成技术，近存计算架构能够提供大的内部内存带宽。
        
    *   **能效提升**：减少数据在存储和计算单元间的传输，降低功耗。
        
    *   **响应速度提升**：减少数据搬运的延迟，使得实时数据分析和响应成为可能。
        

### 应用场景

*   **人工智能与深度学习**：在神经网络和深度学习模型的训练和推理中，PIM和NMC可以极大地提高计算效率和能效，减少训练时间和能源消耗。
    
*   **边缘计算**：在资源受限的边缘设备上，PIM和NMC能够实现低功耗、高处理能力的计算任务，适合物联网（IoT）、自动驾驶汽车、无人机等场景。
    
*   **高性能计算（HPC）**：在科学计算、工程模拟等领域，PIM和NMC有助于加速数据密集型任务的处理。
    
*   **数据库与数据分析**：PIM和NMC可以显著提高数据库查询速度和数据分析效率，特别是在处理海量数据时。
    

在大模型（LLM）推理的解码阶段，面临着所谓的“内存墙”问题，这主要是因为其算术强度较低。这一问题并不新鲜，计算机架构领域已经与“内存墙”问题斗争了几十年。在各种潜在解决方案中，近年来内存中处理（Processing-in-Memory, PIM）技术引起了极大关注。通过直接在内存芯片中放置计算单元，我们可以利用更高的内部内存带宽，减少数据在内存和CPU/GPU核心之间移动的开销。

近年来，基于DRAM的PIM技术已进入商业化阶段，有可能缓解LLM推理的内存带宽瓶颈。如表2所示，UPMEM的PIM-DIMM Devaux是首个商业化的DRAM-PIM产品，它在DDR4-DIMM中嵌入了通用RISC核心。然而，该产品并非为深度学习应用设计，因此峰值带宽和吞吐量难以满足LLM推理的需求。与UPMEM的PIM-DIMM相比，三星提议将MAC单元置于HBM内存中，实现了2TB/s的内部内存带宽，远高于传统HBM2（每个立方体307GB/s）的带宽。由于处理单元是针对深度学习负载定制的，HBM-PIM的峰值计算吞吐量可达到1.2TFLOPS。换句话说，HBM-PIM适合加速具有1-2 Ops/Byte算术强度的操作。

Choi等人提出使用HBM-PIM加速KV缓存处理，这在批处理的LLM推理中算术强度较低。根据他们的评估,GPU+HBM-PIM系统进行LLM推理可以比传统的单一GPU系统快3.24倍。与三星的HBM-PIM类似，SK-hynix也提出了一个基于GDDR6的PIM加速器Kwon等人称为AiM。如表2所示，AiM的计算单元采用BF16数据格式，这对于深度学习加速更为高效。通过优化的MAC单元，AiM每芯片提供1TFLOPS的计算容量，而峰值带宽为每芯片1TB/s。尽管AiM尚未报告其在LLM上的性能，但在LSTM任务上，它相对于GPU+HBM2系统可实现高达10倍的速度提升。

需要注意的是，尽管基于DRAM的PIM技术在展示加速LLM推理中内存密集型操作的潜力方面表现出了良好前景，但仍有一些限制需要在未来解决。

有限的计算能力。DRAM-PIMs在加速大型语言模型（LLMs）方面的主要限制之一是其计算能力受到约束。DRAM-PIMs利用采用DRAM工艺制造的计算单元，导致晶体管速度比同一技术节点下的CMOS慢3倍，逻辑密度也低数倍。更糟糕的是，DRAM芯片通常具有较少的金属层，这同时导致了较低的布线密度。由于这些技术限制，DRAM-PIMs很难整合强大的计算单元。因此，DRAM-PIMs仅适用于小批量推理或键值缓存处理。对于计算密集型的大批量推理，仍然需要一个强大的主机。

容量约束。DRAM-PIMs的另一个重要限制是其有限的容量。由于DRAM-PIMs需要分配部分内存容量来构建计算单元，因此总内存容量通常比标准内存少50%左右，如Kwon等人所述。对于大型语言模型（LLM）应用而言，由于需要大量内存容量来存储模型权重和KV缓存，DRAM-PIMs可能会遇到与容量相关的挑战。

内存内部通信不足。除了计算能力和容量的限制之外，DRAM-PIM（处理器内存储器）的另一个局限性在于它们欠佳的内存单元间通信能力。考虑到每个DRAM存储库附近都分布有计算单元，数据聚合和计算同步在这些单元之间是不可避免的。然而，DRAM-PIM缺乏强大的互连技术，如Jonatan等人和周等人2023d\]所述，它们通常依赖于主机的CPU或GPU来在PIM单元之间交换数据。这种依赖可能导致系统效率低下。因此，为了提升大型语言模型（LLM）的推理性能，DRAM-PIM的未来迭代应旨在增强其内存单元间的通信能力。

## 3. New Processing Element (Sec.6.3)

神经网络在训练中通常使用高精度浮点数（如16或32位）。虽然高精度浮点数既能保证表示的精确度又能涵盖广泛的数值范围，但实现浮点运算所需的复杂硬件不利于高效的推理。为了减轻硬件开销，统一量化将高精度浮点数转换为低精度的整数表示，用高效的整数逻辑替代昂贵的浮点逻辑。然而，统一量化难以同时平衡表示的精确度和范围，导致模型准确性大幅下降。此外，要保持模型准确性不降低，就需要精心设计的量化算法，这又引入了额外的转换工作。非统一量化试图通过非均匀地分配位数和离散参数的范围，在低比特条件下增强数据表示的精度。然而，非统一量化的一个主要缺点是它在通用计算硬件（如CPU和GPU）上的部署具有挑战性 Gholami等人。总之，现有的数据格式未能同时实现精细的精度、广泛的数据范围、高效率以及低成本的调整，这对于推理而言尤为关键。鉴于降低大型语言模型（LLMs）部署成本的重要性，大量研究工作致力于探索最适合LLMs的最平衡数据格式。

为了从原始浮点模型中换取更佳的硬件效率，一个自然的发展方向是减少高分辨率浮点格式中的指数和尾数位。近期的工作表明 如Micikevicius等人包括大型语言模型（LLMs）在内的多种类别模型，在FP16中预训练后，可以直接量化到FP8，而不会造成显著的精度下降。此外，在广泛的任务范围内，使用FP8进行训练能够有效达到与16位训练相当的结果质量。低精度浮点格式所带来的硬件效率的巨大提升及对用户最小化的额外要求，已经引起了AI硬件制造商的广泛关注。例如，NVIDIA在其最新的H100 GPU中实现了FP8张量核心 NVIDIA, 2022年\]。特斯拉也在其Tesla Dojo芯片中引入了可配置的浮点格式，即CFloat8 Tesla, 2023年\]。

除了产业界推出的创新架构，学术界也开始了探索低精度浮点格式在大型语言模型上潜力的努力。ZeroQuant-FP Wu等人提出了针对LLMs权重/激活量化的FP4和FP8。作者采用量化时的比例约束，实现了从FP4到FP8的有效权重转换，并更好地利用了FP8张量核心。ZeroQuant-(4+2) Wu等人和FP6-LLM Xia等人则提出了使用FP6对LLMs权重进行量化的方法，并分别在CUDA核心和张量核心上提供了高效的实现。LLM-FP4 Liu等人提出将LLMs的权重和激活同时量化到FP4。综上所述，这些努力证明了应用更低位宽的浮点格式进行量化是可行的，同时也展现了在现有或新硬件平台上实现更大效率提升的潜力。

另一方面，研究人员正在深入研究低精度量化格式的改进，以增强数据表示的适应性，同时保持硬件效率。一类研究工作提议探索单一数值表示内的新编码方案。与使用固定长度子字段来编码诸如指数和尾数等不同信息的INT和FP数字不同，这些新提出的基于规则的量化格式能够动态调整子字段的位宽。ALPS  Langroudi等人提出了一种广义的posit格式以及一个新的自适应量化算法，以最优地表示DNN参数的动态范围和分布。ANT  Guo等人a\]提出了一种新的数据格式称为flint，采用前导1编码用于指数域。Dybit  Zhou等人建议使用首次遇到的0作为分隔符来分离指数和尾数域。这些可变长度数据格式的灵活性提供了在范围和精度之间更有效折中的机会，并允许根据LLMs（大型语言模型）权重和激活的分布进行更紧密的定制。

另一系列工作利用数值之间的相似性和差异性。异常值感知量化技术基于这样的观察：具有较大绝对值的数值对模型性能有显著影响。在这种方法中，将重要数值识别为异常值，并区别于普通数值进行处理，以确保更准确的表示。OLAccel  Park等人和GOBO  Zadeh等人分别存储异常值并为其分配更高的位宽。OliVe  Guo等人通过引入异常值-受影响对编码方案来完善这一概念，确保了对齐的内存访问和更高的效率。比特共享编码着重于数值间的内在相似性，并在较粗的粒度上附加额外信息，从而在表示精度和硬件效率之间取得平衡。AdaptivFloat  Tambe等人提出最优地调整可用浮点数范围，采用一个公共的张量级指数偏移。MX  Darvish Rouhani等人将AdaptivFloat的观察扩展到更细的粒度，并提出了块数据表示（BDR）框架，以探索表示精度和硬件效率之间的最优权衡。

## 4. New Data Format (Sec.6.4)

除了对内存访问的高需求之外，近年来开发专门处理单元（PEs）以增强计算能力的兴趣日益增长。这些专用架构旨在针对与大型语言模型（LLMs）相关的特定操作，相比如CUDA核心这样的通用处理元素，提供显著的计算性能提升。英伟达已经开发了一种特殊的硬件加速引擎，称为Transformer Engine，集成在他们的H100 GPU中。该引擎利用统计分析确定模型每一层的最佳精度（FP16或FP8），从而在保持准确性的前提下实现最佳性能。

一些研究者设计了专门针对高效执行语言模型中注意力机制的加速器 Kao等人；Qin等人。多家公司和研究团队一直在探索使用现场可编程门阵列（FPGA）来加速LLM计算的研究。例如，DFX  Hong等人和LightLLM  Zeng等人，2024\]就是这一领域的实践案例。

**新型数据格式详解（Section 6.4）**

在现代计算和数据处理领域，高效、精确的数据表示方式至关重要。本节将详细介绍几种关键的数据格式，包括BFloat16、TFLOAT、FP\*、Fix8、Float32、Float16以及Int8，旨在为读者提供一个清晰的理解框架。

### 1. BFLOAT16 (Brain Floating Point Format)

*   **简介**：BFLOAT16，也称为Brain Float，是一种计算机科学中的半精度浮点数格式，专为机器学习和人工智能领域的高性能计算设计。它由Google开发，旨在平衡精度与计算效率。
    
*   **位表示**：16位，包括1位符号位，8位指数位，7位尾数位。
    
*   **应用场景**：深度学习模型训练和推理，尤其是在对内存带宽敏感的环境中。
    

### 2. TFLOAT

*   **简介**：TFLOAT没有标准定义，但可能指特定于某个实现或研究中的自定义浮点格式，特别是与TensorFlow或其他机器学习框架相关的优化。具体位布局依赖于具体实现。
    
*   **特点**：通常设计来优化特定硬件的计算效率和存储需求。
    
*   **应用场景**：根据具体实现，可能用于高效的神经网络运算。
    

### 3. FP (Flexible Precision Floating Point)\*

*   **简介**：FP\*是一种灵活精度的浮点数表示方法，允许根据需要调整精度。这并不是一个特定的标准格式，而是一种概念，表明可以根据不同应用场景调整浮点数的位数分配。
    
*   **特点**：动态调整精度，以达到计算效率和数值精度的最佳平衡。
    
*   **应用场景**：适应性强，适用于对精度和效率有动态要求的计算任务。
    

### 4. Fix8 (Fixed Point Format, 8-bit)

*   **简介**：Fix8是一种8位定点数格式，不使用指数表示法，所有位都直接贡献于数值表示，适合低成本、低功耗的嵌入式系统。
    
*   **位表示**：通常包括1位符号位和7位数值位，或者根据需要调整符号位和数值位的分配。
    
*   **应用场景**：物联网设备、传感器数据处理、基本的音频处理等对成本和功耗敏感的场景。
    

### 5. Float32 (Single Precision Floating Point)

*   **简介**：IEEE 754标准下的单精度浮点数格式，广泛应用于科学计算、图形处理等领域。
    
*   **位表示**：32位，包括1位符号位，8位指数位，23位尾数位。
    
*   **应用场景**：通用计算，特别是在需要较高精度的科学计算和图形渲染中。
    

### 6. Float16 (Half Precision Floating Point)

*   **简介**：同样遵循IEEE 754标准，是单精度浮点数的一半大小，用于节省内存和提高计算速度。
    
*   **位表示**：16位，包括1位符号位，5位指数位，10位尾数位。
    
*   **应用场景**：深度学习模型训练的中间结果存储、移动设备上的图形处理等对存储和带宽敏感的场景。
    

### 7. Int8 (8-bit Integer Format)

*   **简介**：8位有符号或无符号整数格式，是最基本和最常见的整数类型之一。
    
*   **位表示**：8位全部用于数值表示，最高位作为符号位（有符号时）。
    
*   **应用场景**：图像处理、机器学习算法中的权重量化、压缩存储等，尤其在深度学习推理中用于降低模型大小和加速计算。
    

这些数据格式各有特色，选择合适的格式需根据具体应用的需求来决定，如精度要求、计算资源限制、存储需求等因素。

## FP8

[https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#quantization-schemes: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#quantization-schemes](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#quantization-schemes)

## 5.芯片工艺

### 3D Dram堆叠

### HBM