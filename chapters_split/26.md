# 优化大模型应用的时间延迟，提高反应速度


## 语音识别（ASR）优化

1.  **选择高效模型**
    
    *   使用轻量且快速的ASR模型（如Wav2Vec 2.0、Conformer等）
        
    *   选择针对特定领域训练过的语音识别引擎
        
2.  **流式识别技术**
    
    *   实现增量式/流式语音识别，边说话边转录
        
    *   实时传递已识别部分文本，无需等待完整语音输入
        
3.  **硬件与输入优化**
    
    *   利用GPU或专用AI加速器提升计算效率
        
    *   优化语音采集质量，减少环境噪音干扰
        
    *   应用降噪算法提高语音清晰度
        

## 大模型推理优化

1.  **模型架构简化**
    
    *   使用专为低延迟设计的模型变体（如Haiku类模型）
        
    *   减少层数、注意力头数或中间维度大小
        
2.  **模型压缩技术**
    
    *   **量化**：将FP32/FP16精度降为INT8、INT4甚至二值化
        
    *   **剪枝**：删除模型中不重要的参数，减小规模
        
    *   **知识蒸馏**：将大模型知识转移到更小、更快的模型
        
3.  **推理引擎优化**
    
    *   使用TensorRT、ONNX Runtime或vLLM等高性能推理引擎
        
    *   实现kernel fusion减少内存访问和同步点
        
4.  **并行与分布式计算**
    
    *   模型并行或数据并行方式分配到不同计算节点
        
    *   张量并行、流水线并行等技术
        
5.  **缓存与预计算**
    
    *   对相同或相似输入缓存推理结果，避免重复计算
        
    *   复用中间计算结果减少计算量
        
6.  **流式推理优化**
    
    *   逐步推理（Incremental Inference）：分段处理输入文本
        
    *   预填充与推理分离：将prompt处理与生成过程分离
        

## 文本转语音（TTS）优化

1.  **选择高效TTS模型**
    
    *   使用高效神经网络模型（如Tacotron 2、FastSpeech 2、WaveNet等）
        
    *   选择在特定领域优化过的TTS引擎
        
2.  **流式TTS技术**
    
    *   实现流式TTS，边接收文本边合成语音
        
    *   文本生成一部分就立即开始合成对应的语音
        
3.  **预生成与缓存**
    
    *   对常用文本内容预先合成并缓存
        
    *   并行合成：将长文本分割成小段并行处理
        

## 系统级优化

1.  **内存管理**
    
    *   实现连续批处理（continuous batching）
        
    *   优化KV缓存内存布局和管理策略
        
2.  **异步与管道化处理**
    
    *   将ASR、推理和TTS组成异步流水线
        
    *   各模块并发执行而非顺序等待
        
    *   中间结果流式传输，无需等待前一步骤完全结束
        
3.  **资源分配与负载均衡**
    
    *   合理调度CPU、GPU和内存资源
        
    *   通过负载均衡均匀分配请求，避免单点过载
        

## 网络与部署优化

1.  **边缘计算**
    
    *   将计算任务部署在靠近用户的设备或边缘服务器
        
    *   减少网络传输时间和延迟
        
2.  **网络传输优化**
    
    *   使用低延迟网络协议（如UDP代替TCP）
        
    *   减少模块间数据传输延迟，如使用内存共享或快速IPC
        
    *   采用CDN加速数据传输
        
3.  **混合架构部署**
    
    *   本地小模型 + 云端大模型协作
        
    *   简单请求由本地模型处理，实现毫秒级响应
        
    *   复杂请求路由至云端大模型
        

## 用户体验优化

1.  **流式响应机制**
    
    *   生成第一个token后立即返回给用户
        
    *   使用WebSocket或SSE实现高效前后端通信
        
2.  **渐进式UI更新**
    
    *   优先生成并展示最关键的信息部分
        
    *   让用户感知到系统正在处理，减少感知延迟
        
3.  **预测性加载**
    
    *   预测用户可能的下一步操作并预加载资源
        
    *   模型预热：系统启动时预先加载和初始化模型
        

## 高级优化策略

1.  **投机执行**
    
    *   实现并行解码，同时生成多个可能的后续token
        
    *   预测用户可能的后续查询并提前准备
        
2.  **端到端优化**
    
    *   通过性能监控工具找出流程中的瓶颈环节
        
    *   优化各环节之间的衔接和协同工作
        

通过综合应用这些优化技术，可以将大模型应用的响应时间降至200-500毫秒级别，达到或超过人类反应速度（约250毫秒），使交互体验更加流畅自然。

4.11-bit LLM的时代:BitNet

4.2 权重的量化Weight Quantization

4.3激活值的量化ActivationQuantization

4.4反量化过程Dequantization

4.5所有 LLMs实际上均为1.58-bit

4.5.1 The Power of 0

4.5.2Quantization量化过程

Gemma on-device with MediaPipe

LLM性能指标有

*   **Time to First Token (TTFT)**：prefilling阶段耗时
    
*   **Time-Per-Output-Token (TPOT)**：每次decoder阶段平均耗时
    
*   **Latency**：生成总耗时=TTFT+TPOT\*生成的token数目
    
*   **throughput**：生成的token数目/Latency
    
*   **Model memory**：模型内存
    
*   **Peak Momery**：峰值内存=模型内存+kvcache
    

[https://zhuanlan.zhihu.com/p/699776257?utm\_psn=1816789257005719552](https://zhuanlan.zhihu.com/p/699776257?utm_psn=1816789257005719552)

A Survey on Efficient Inference for Large Language Models

1、数据优化

2、模型级别优化

2.1、架构优化

2.2、量化

3、系统级别优化

Prefill vs decoding

DistServe多卡解耦prefill和decode优化

预测解码、多token解码等

KV Cache Compression

draft模型和原始模型联合推理

continues batching/In-Flight Batching

FlashAttention分析

代码逻辑

flash\_fwd\_kernel参数

attn的tile计算核心代码

FlashDecoding

PagedAttention

cuda代码实现

总结

相关文章

本文主要内容

介绍一篇大模型推理加速综述论文，简单说明了LLM推理加速的基本内容。

介绍了推理阶段的prefilling（主要方向：计算优化）和decoding（主要方向：内存优化）差异。

prefilling优化方面，针对流行的FlashAttention和PagedAttention和Lmdeploy等优缺点进行了详细讲解。

decoding优化方面，主要有多token预测、降低kv cache等方法。

## awq：

[https://mp.weixin.qq.com/s/7tPKmp-Z\_unsjl7n7lV89Q](https://mp.weixin.qq.com/s/7tPKmp-Z_unsjl7n7lV89Q)

https://www.doubao.com/thread/we1232d090f99babb

融合内核（Fused kernels）将这些计算步骤整合，减少中间结果写入和读取内存的次数。在内存受限的边缘设备上，频繁的内存访问会成为瓶颈，融合内核通过减少数据搬运开销，提升计算效率，使受内存限制的边缘 LLM 运行更高效

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/7dbc4dd9-ca9c-466b-95f1-2041e19e20ae.png)

以下是关于AWQ量化技术的核心总结：

### AWQ量化技术概述

*   **定义**：AWQ（Activation-aware Weight Quantization）是一种基于激活值分布筛选显著权重进行量化的训练后量化（PTQ）方法。其核心思想是通过保留关键权重的精度，对其他权重进行低比特量化，在保持模型精度的同时降低内存占用和提升推理速度。
    
*   **应用**：已被集成至TensorRT-LLM、vLLM等主流推理框架，并被NVIDIA、Google等厂商采用，获MLSys 2024最佳论文提名。
    

---

### 核心观点

1.  **权重重要性差异**：
    
    *   仅0.1%~1%的显著权重对模型输出影响较大，需保留高精度（如FP16）。
        
    *   **筛选方法**：基于激活值分布（而非权重绝对值或随机选择）选择显著权重，显著降低量化误差（实验表明PPL指标接近FP16）。
        
2.  **缩放（Scaling）技术**：
    
    *   对显著权重通道进行放大，降低其量化误差；非显著通道缩小以减少关注。
        
    *   **实现方式**：统计激活值各列绝对值的均值（`Sx`）作为缩放系数，通过网格搜索优化平衡参数（`α`），最终确定最佳缩放系数。
        

---

### 技术细节

*   **通道级量化**：以权重矩阵的通道（行）为单位筛选显著权重，而非单个元素，简化存储和计算。
    
*   **Fast Grid Search**：在\[0,1\]区间均匀采样20个点，计算不同`α`下的量化误差（MSE），选择最优解。
    
*   **硬件友好性**：所有权重量化为低比特（如INT4），通过缩放系数反量化计算，避免混合精度存储的复杂性。
    

---

### 优势与误区

*   **优势**：
    
    *   精度损失极小（接近FP16），内存占用降低，推理速度提升。
        
    *   泛化性强，不过度依赖校准集，适用于多领域任务。
        
*   **误区澄清**：
    
    *   AWQ ≠ W4A16：AWQ支持多种量化配置（如W4A8、W3A16），W4A16仅为常用方案。
        

---

### 应用与争议

*   **部署**：支持W4A16、W4A4等配置，平衡精度与效率。
    
*   **争议点**：部分权重（如LLAMA的`W_o`）是否需量化，需参考具体实现。
    

---

### 总结

AWQ通过激活值感知的权重选择和缩放技术，在量化过程中智能保留关键信息，实现了高效、低损的模型压缩，成为大模型部署中的重要优化手段。

## [A Survey on Efficient Inference for Large Language Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2404.14294)

Transformer模块主要由Attention+MLP/MOE组成。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/777d7613-6c3a-409a-969f-89c209c45b17.webp)attention

prefilling：大模型计算全部的prompt，生成第一个token，并存储所有的KV缓存。

decoding：输入前面生成的单个token，利用KV缓存一起计算出下一个token，将当前token计算出的新kv值添加到kv缓存队列中。循环当前步骤，直至当前输出的token等于截至token或者生成的token总数目到达输出上限。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/4f4ede89-e5c1-4959-9296-0e74a465f6de.webp)llm inference

存储kv cache是为了避免重复计算。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/7405f7e5-934b-4f41-8ec1-d5d3e885d3b2.webp)

LLM性能指标有

Time to First Token (TTFT)：prefilling阶段耗时

Time-Per-Output-Token (TPOT)：每次decoder阶段平均耗时

Latency：生成总耗时=TTFT+TPOT\*生成的token数目

throughput：生成的token数目/Latency

Model memory：模型内存

Peak Momery：峰值内存=模型内存+kvcache

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/b5f0e086-03c0-477d-a027-18d8f218e949.webp)LLM推理时的Memory和Latency变化

问题：随着context的增加，内存消耗和计算速度都会平方级增加。

模型在小bs\*短seq情况下，会出现传输瓶颈，当数据量增大到一定程度，会碰到计算瓶颈。

[极市开发者平台-计算机视觉算法开发落地平台-极市科技](https://link.zhihu.com/?target=https%3A//www.cvmart.net/community/detail/8245)

推理优化

数据优化：prompt总结、压缩、检索

模型优化：1、GQA、GLA、MOE等；2、量化、蒸馏等

系统优化：vllm、lmdeploy、tensorRT等

### 1、数据优化

prompt总结、压缩、检索（RAG）。其中RAG用的最多，其他都用的很少。

[akaihaoshuai：从0开始实现LLM：4、长上下文优化（理论篇）](https://zhuanlan.zhihu.com/p/683731440)

### 2、模型级别优化

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/0553146a-e509-4cdf-a085-1d4aac681813.webp)模型压缩方法总览

### 2.1、架构优化

架构上来说，主要是针对Attention和FFN进行优化。

Attention：MHA->MQA->GQA->MLA等。

FFN->MOE

模型的耗时分布如下图所示

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/9518ce9c-0690-417a-b2cf-75297618c14f.webp)

推理耗时

FFN在大模型中贡献了很大一部分模型参数，这导致显著的内存访问成本和内存使用，特别是在解码阶段。例如，FFN模块在LLaMA-7B模型中占63.01%，在LLaMA-70B模型中占71.69%。

还有稀疏Attention、线性Attention，其他更大的架构变化还有[KAN](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2404.19756)、[megalodon](https://link.zhihu.com/?target=https%3A//github.com/XuezheMax/megalodon)、[mamba](https://link.zhihu.com/?target=https%3A//github.com/state-spaces/mamba)等

架构调整需要重新训练LLM，耗时耗力，只有头部公司在做。剩下99%的公司都在pretrained model上做SFT和RLHF，无法修改预训练模型的结构。  

剩下比较好用的就是量化、剪枝、蒸馏等。其中相对来说量化最成熟，用的最多的有GPTQ和AWQ。

### 2.2、量化

[akaihaoshuai：从0开始实现LLM：6、模型量化理论+代码实战（LLM-QAT/GPTQ/BitNet 1.58Bits/OneBit）](https://zhuanlan.zhihu.com/p/686161543)

量化可以大幅减少模型体积，但由于引入了反量化操作，计算量有所增加，在小bs+短token场景下（传输瓶颈），速度更快。在大bs和超长token场景下（计算瓶颈），反而比非量化更慢。  

其中vllm中的量化效果一般，Lmdeploy的量化速度很快。LMdeploy加速效果略好于TensorRT-LLM。底层都是基于[FasterTransformer](https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/FasterTransformer)。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/0d9945d0-8573-4c37-a98b-5f3bfbdf87f9.webp)

由图可看出，prefill阶段加速比基本在0.9~1之间，变化不大。decoding阶段加速比可在2倍左右。  

### 3、系统级别优化

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/8fc0e86f-c87f-4754-b71e-7aafdb0d7af7.webp)

系统级别优化是工程性优化，主要是算子合并、加速、多卡并行、内存管理等方面进行优化。主要有FlashAttention、PagedAttention、FlashDecoding和continues batching等。后面会详细讲解。  

### O1 推理

强化学习，算力提高

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/e32cb030-b42f-415a-b6b0-38ab7fcea98f.png)

## Prefill vs decoding

以 Llama2-7B（4096 序列长度，float16精度）为例，计算一下 batch\_size = 1的理想推理速度。

Prefilling：假设 prompt 的长度是 350 token，那么预填充所需要的时间 = number of tokens \* ( number of parameters / accelerator compute bandwidth) = 350 \* (2 \* 7B) FLOP / 125 TFLOP/s = 39 ms（A10)。这个阶段主要是计算瓶颈。

decoding：time/token = total number of bytes moved (the model weights) / accelerato r memory bandwidth = (2 \* 7B) bytes / (600 GB/s) = 23 ms/token（A10)。这个阶段的瓶颈是带宽。

[大语言模型（LLM）推理性能优化以及推理框架、后端的评测 · Issue #107 · ninehills/blog](https://link.zhihu.com/?target=https%3A//github.com/ninehills/blog/issues/107)

prefilling阶段的计算效率要高很多。prefilling数据量较大，更容易遇到计算瓶颈，这时候的优化方向是算子合并、简化等，降低模型计算量。

decoding时query长度为1，数据量小，更容易遇到传输瓶颈，这时候的优化主要为kv cache的访问优化，比如tile计算和cache量化等。

[回旋托马斯x：FlashAttention:加速计算,节省显存, IO感知的精确注意力](https://zhuanlan.zhihu.com/p/639228219)

### DistServe多卡解耦prefill和decode优化

prefilling阶段对计算资源的需求量极大，哪怕是小批次的预填充任务，甚至单个较长的预填充任务，都足以使GPU的计算能力达到饱和。与此相对，解码任务则需要更大的批大小才能充分利用计算资源。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/1357fd67-e89a-400a-bd69-ec37e31a6f8d.webp)

如上图所示，连续批处理任务中，prefill任务R2的加入，显著增加了decoding任务（R1）的时延，并略微增加了预填充任务（R2）的时延。处于解码阶段的请求在每次预填充请求进入系统时都会“卡住”，因此意外地增加了解码任务的时延。  

因此，将预填充和解码解耦到不同的GPU中，并为每个阶段定制并行策略。这自然解决了上述两个问题：

预填充和解码之间没有干扰，使得两个阶段都能更快地达到各自的SLO。

资源分配和并行策略解耦，从而为预填充和解码量身定制优化策略。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/0876c1a3-8f72-4105-b35f-ff8bfe529f20.webp)

[Serving有效吞吐量的最大化实现](https://link.zhihu.com/?target=https%3A//my.oschina.net/oneflow/blog/11127355)

### 预测解码、多token解码等

既然decoding的多次迭代耗时，那么自然而然的一个想法就是减少迭代次数。所以有个下面这篇一次预测多个token的论文。（需要修改模型架构）

[next-token被淘汰！Meta实测多token训练方法，推理提速3倍](https://link.zhihu.com/?target=https%3A//www.toutiao.com/article/7376159248470655527/%3Fapp%3Dnews_article%26group_id%3D7376159248470655527%26req_id%3D20240604003754357885CCD4C954C5662D%26share_token%3D60fa138b-d713-461c-b951-99700c9f35ae%26timestamp%3D1717432675%26tt_from%3Dcopy_link%26use_new_style%3D1%26utm_campaign%3Dclient_share%26utm_medium%3Dtoutiao_android%26utm_source%3Dcopy_link%26source%3Dm_redirect)

根据模型参数越小，速度越快。prefill速度比多次decode更快的结论。也可以通过小模型对prompt生成结果（prefilling+decoding），再将prompt+结果一起输入大模型（prefilling），达到加速目的。（ps：这个不一定能快多少，但是用不好的话会慢不少。。。）

[Accelerating Large Language Model Decoding with Speculative Sampling](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.01318)

### KV Cache Compression

[https://github.com/opengear-project/GEAR/tree/main](https://link.zhihu.com/?target=https%3A//github.com/opengear-project/GEAR/tree/main)

[GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.05527)

GEAR通过三种互补技术来分解和压缩KV矩阵：

使用统一量化将大部分相似幅度的条目压缩到极低精度（如4位）。

使用低秩矩阵来有效近似量化残差。

引入稀疏矩阵来纠正异常条目的个别误差。

内存压缩可以降低kvcache内存，提高decoding效率。

### draft模型和原始模型联合推理

创建一个小的draft模型，借助draft模型快速推理初始结果，将prompt和初始结果一起输入原始大模型做判别，充分利用了小模型速度快，prefill阶段比decoding阶段计算效率高的特点，也可以对模型进行加速。

[Paper page - Distributed Speculative Inference of Large Language Models](https://link.zhihu.com/?target=https%3A//huggingface.co/papers/2405.14105)

## continues batching/[In-Flight Batching](https://zhuanlan.zhihu.com/p/679723881)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/86124ca3-2088-4fcc-98d8-b8579af738d7.webp)

[Continuous Batching：一种提升 LLM 部署吞吐量的利器](https://link.zhihu.com/?target=https%3A//www.high-flyer.cn/blog/continuous-batching/)

现实情况比这个简化模型更复杂：因为预填充阶段需要计算，并且与生成阶段的计算模式不同，因此它不能很容易地与令牌的生成一起进行批量。 vllm的实现为：每次迭代前都会从request数据中查找还在运行的request和新收到的request，整理到一个大batch中。每次迭代结束会判断是否有request满足结束条件，若满足则标记为finished，不再处理。 由于不同batch的状态不同，为了减少pad的内存和计算浪费，最终处理时会将其整理成bs=1的超长total\_seq序列，因为根据矩阵乘法，无论是(bs, seq)还是(1, bs\*seq)，linear层结果均完全相同，LN层也是。

## FlashAttention分析

[GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://link.zhihu.com/?target=https%3A//github.com/Dao-AILab/flash-attention)

通过tile计算降低attention模块的数据传输量，提高模块效率。其cuda实现针对大batchsize有明显提升，小bs场景下提升不明显。详细原理网上有很多教程，这里不再赘述。

主要在于矩阵的拆分和softmax的拆分。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/8f2815f2-153a-4fc8-871d-2c38900d5188.webp)

[flash attention V1 V2 V3 V4 如何加速 attention](https://zhuanlan.zhihu.com/p/685020608?utm_psn=1747635933258833920)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/f458b6a7-c1f1-4296-883a-6072b34f9389.webp)

GPU内存图如下。从共享内存中访问数据比从全局内存中访问要快很多。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/b6c696dd-506f-4659-869f-2593026e5be2.webp)

具体计算公式推导可参考：[如何评价flashattention最新更新flash decoding，推理性能提升8倍？](https://www.zhihu.com/question/626100703/answer/3442383228)

### 代码逻辑

vllm只在所有的prompt都位于prefilling阶段时使用FlashAttention，在decoding阶段使用PageAttention（因为FlashAttention无法访问不连续的kvcache）。  

vllm输入时会将所有的seq合并到一起，并记录每一个prompt的起始位置、生成token的长度等状态。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/bd792f3d-7f68-415c-b9af-bc08e3a5f404.webp)

这里判断当block\_tables为0，也就是全部数据都没有kvcache时，才能使用FlashAttention。  

在flash\_attn\_varlen\_func()->\_flash\_attn\_varlen\_forward()函数中，也有要求输入数据qkv的内存连续。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/8ae87c4c-cf31-4946-831d-3c3036feda6a.webp)

\_flash\_attn\_varlen\_forward调用flash\_attn\_cuda.varlen\_fwd()==mha\_varlen\_fwd()函数，参数备注上有说明，参数的第一个维度是total\_len，也就是合并seq的长度。

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/d5ed9a24-4c3f-412d-9076-1f09a147ed9b.webp)您好，您提到之前的解释“不够细致”，我理解您希望得到一个更深入、更详细的解答。为了满足您的需求，我将以一次性计算 KV 缓存的过程为例，尽可能拆解每个步骤，提供详细的解释，并结合数学公式和具体示例，让您对这个过程有更清晰的理解。以下是我的完整解答。

---

## 一次性计算 KV 缓存的详细过程

在 Transformer 模型中，尤其是自回归生成任务（如语言模型生成文本）中，一次性计算并缓存输入序列的 Key（K）和 Value（V）是一个重要的优化手段。这种方法通过预先计算和存储输入序列的 K 和 V，避免了在生成后续 token 时重复计算历史信息，从而显著提升效率。下面，我将逐步拆解这个过程，并提供详细的数学推导和例子。

### 1. 分词与嵌入（Tokenization and Embedding）

#### 1.1 分词

*   **过程**：假设输入文本是“今天天气怎么样”，我们需要将其分解成单独的 token。分词器根据预训练的词汇表，将文本拆分为以下 token：
    
    *   "今天"
        
    *   "天气"
        
    *   "怎么样"
        
*   **细节**：每个 token 会被映射到一个唯一的整数 ID。例如：
    
    *   "今天" → 1001
        
    *   "天气" → 1002
        
    *   "怎么样" → 1003
        
*   **输出**：一个长度为 3 的 token ID 序列：\[1001, 1002, 1003\]。
    

#### 1.2 嵌入

*   **过程**：通过嵌入层（Embedding Layer），将每个 token ID 转换为高维向量。假设模型的嵌入维度 \( d\_{\text{model}} = 512 \)。
    
*   **数学表示**：
    
    *   嵌入矩阵 \( E \in \mathbb{R}^{|V| \times 512} \)，其中 \( |V| \) 是词汇表大小（例如 \( |V| = 50,000 \)）。
        
    *   对于每个 token ID，嵌入向量为：
        
        *   \( x\_1 = E\[1001\] \in \mathbb{R}^{512} \)（“今天”的嵌入）
            
        *   \( x\_2 = E\[1002\] \in \mathbb{R}^{512} \)（“天气”的嵌入）
            
        *   \( x\_3 = E\[1003\] \in \mathbb{R}^{512} \)（“怎么样”的嵌入）
            
*   **输出**：输入序列的嵌入张量： \\[ X = \begin{bmatrix} x\_1 \\ x\_2 \\ x\_3 \end{bmatrix} \in \mathbb{R}^{3 \times 512} \\]
    

### 2. 加入位置编码（Positional Encoding）

由于 Transformer 模型本身无法感知 token 的顺序，我们需要为每个 token 添加位置信息。

*   **过程**：为每个 token 添加一个位置编码向量，表示其在序列中的位置（这里是 0、1、2）。
    
*   **数学表示**：位置编码通常使用正弦和余弦函数生成。对于位置 \( pos \)（从 0 到 2）和维度 \( i \)（从 0 到 255，假设 \( d\_{\text{model}} = 512 \)），计算公式为： \\[ P(pos, 2i) = \sin\left(\frac{pos}{10000^{2i / 512}}\right), \quad P(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i / 512}}\right) \\]
    
*   **具体计算**：
    
    *   对于 \( pos = 0 \)（“今天”），生成 \( p\_1 \in \mathbb{R}^{512} \)
        
    *   对于 \( pos = 1 \)（“天气”），生成 \( p\_2 \in \mathbb{R}^{512} \)
        
    *   对于 \( pos = 2 \)（“怎么样”），生成 \( p\_3 \in \mathbb{R}^{512} \)
        
*   **输出**：位置编码张量 \( P \in \mathbb{R}^{3 \times 512} \)，更新后的输入为： \\[ \tilde{X} = X + P = \begin{bmatrix} x\_1 + p\_1 \\ x\_2 + p\_2 \\ x\_3 + p\_3 \end{bmatrix} \in \mathbb{R}^{3 \times 512} \\]
    

### 3. 多头注意力机制中的 KV 计算

Transformer 使用多头注意力机制，每个注意力头独立计算 Query（Q）、Key（K）和 Value（V）。一次性计算 KV 缓存的核心就在这一步。

*   **参数假设**：
    
    *   注意力头数 \( h = 8 \)
        
    *   每个头的维度 \( d\_k = d\_v = 64 \)（因为 \( d\_{\text{model}} = 512 \)，\( 512 / 8 = 64 \)）
        
*   **权重矩阵**：
    
    *   \( W\_K^{(i)} \in \mathbb{R}^{512 \times 64} \)：第 \( i \) 个头的 Key 变换矩阵
        
    *   \( W\_V^{(i)} \in \mathbb{R}^{512 \times 64} \)：第 \( i \) 个头的 Value 变换矩阵
        

#### 3.1 计算 Key（K）

*   **过程**：对每个注意力头 \( i \)（\( i \) 从 1 到 8），将输入序列 \( \tilde{X} \) 转换为 Key 矩阵。
    
*   **数学表示**： \\[ K^{(i)} = \tilde{X} W\_K^{(i)} \in \mathbb{R}^{3 \times 64} \\]
    
*   **具体计算**：
    
    *   \( k\_1^{(i)} = (\tilde{x}\_1) W\_K^{(i)} \in \mathbb{R}^{64} \)（“今天”的 Key）
        
    *   \( k\_2^{(i)} = (\tilde{x}\_2) W\_K^{(i)} \in \mathbb{R}^{64} \)（“天气”的 Key）
        
    *   \( k\_3^{(i)} = (\tilde{x}\_3) W\_K^{(i)} \in \mathbb{R}^{64} \)（“怎么样”的 Key）
        
*   **输出**： \\[ K^{(i)} = \begin{bmatrix} k\_1^{(i)} \\ k\_2^{(i)} \\ k\_3^{(i)} \end{bmatrix} \in \mathbb{R}^{3 \times 64} \\] 对 8 个头，生成 8 个 \( K^{(i)} \)。
    

#### 3.2 计算 Value（V）

*   **过程**：对每个注意力头 \( i \)，将输入序列 \( \tilde{X} \) 转换为 Value 矩阵。
    
*   **数学表示**： \\[ V^{(i)} = \tilde{X} W\_V^{(i)} \in \mathbb{R}^{3 \times 64} \\]
    
*   **具体计算**：
    
    *   \( v\_1^{(i)} = (\tilde{x}\_1) W\_V^{(i)} \in \mathbb{R}^{64} \)（“今天”的 Value）
        
    *   \( v\_2^{(i)} = (\tilde{x}\_2) W\_V^{(i)} \in \mathbb{R}^{64} \)（“天气”的 Value）
        
    *   \( v\_3^{(i)} = (\tilde{x}\_3) W\_V^{(i)} \in \mathbb{R}^{64} \)（“怎么样”的 Value）
        
*   **输出**： \\[ V^{(i)} = \begin{bmatrix} v\_1^{(i)} \\ v\_2^{(i)} \\ v\_3^{(i)} \end{bmatrix} \in \mathbb{R}^{3 \times 64} \\] 对 8 个头，生成 8 个 \( V^{(i)} \)。
    

### 4. 存储 KV 缓存

*   **过程**：将所有注意力头的 \( K^{(i)} \) 和 \( V^{(i)} \)（\( i = 1 \) 到 8）存储到内存中，作为缓存。
    
*   **存储细节**：
    
    *   每层的每个注意力头都有独立的 KV 缓存。
        
    *   以 32 位浮点数（4 字节）为例，计算缓存大小：
        
        *   一个 \( K^{(i)} \)：\( 3 \times 64 \times 4 = 768 \) 字节
            
        *   一个 \( V^{(i)} \)：\( 768 \) 字节
            
        *   一个头的 KV 总大小：\( 768 + 768 = 1536 \) 字节
            
        *   8 个头：\( 8 \times 1536 \approx 12 \) KB
            
        *   若模型有 32 层：\( 32 \times 12 \approx 384 \) KB
            
*   **目的**：在后续生成阶段，直接复用这些缓存，避免重新计算历史 token 的 K 和 V。
    

### 5. 生成阶段：复用 KV 缓存

假设模型需要生成回答“很好”（2 个 token），我们来看如何利用 KV 缓存。

#### 5.1 生成“很”

*   **过程**：
    
    *   输入“很”的嵌入和位置编码，计算其 Query： \\[ Q^{(i)} = \tilde{x}\_{\text{很}} W\_Q^{(i)} \in \mathbb{R}^{1 \times 64} \\]
        
    *   使用缓存中的 \( K^{(i)} \in \mathbb{R}^{3 \times 64} \) 和 \( V^{(i)} \in \mathbb{R}^{3 \times 64} \)，计算注意力： \\[ \text{Attention}(Q^{(i)}, K^{(i)}, V^{(i)}) = \text{softmax}\left(\frac{Q^{(i)} (K^{(i)})^T}{\sqrt{64}}\right) V^{(i)} \\]
        
    *   输出用于预测“很”。
        
*   **更新缓存**：计算“很”的 \( K^{(i)} \) 和 \( V^{(i)} \)，追加到缓存中，更新为： \\[ K^{(i)} \in \mathbb{R}^{4 \times 64}, \quad V^{(i)} \in \mathbb{R}^{4 \times 64} \\]
    

#### 5.2 生成“好”

*   **过程**：重复上述步骤，计算“好”的 \( Q^{(i)} \)，利用更新后的缓存计算注意力，预测“好”。
    
*   **更新缓存**：追加“好”的 \( K^{(i)} \) 和 \( V^{(i)} \)，缓存扩展到 5 个 token。
    

#### 5.3 生成更长回答（例如 10 个 token）

*   **过程**：对于每个新 token，重复上述步骤：
    
    *   计算新 token 的 \( Q^{(i)} \)、\( K^{(i)} \)、\( V^{(i)} \)
        
    *   复用历史缓存计算注意力
        
    *   追加新 \( K^{(i)} \) 和 \( V^{(i)} \) 到缓存
        
*   **效率**：每次只需计算 1 个 token 的 Q、K、V，复用 \( n \) 个历史 token 的 KV，避免了 \( O(n^2) \) 的重复计算。
    

---

## 总结

一次性计算 KV 缓存的核心是通过预先计算并存储输入序列的 Key 和 Value，避免在生成阶段重复计算历史信息。无论是生成短回答（如“很好”）还是长回答（如 10 个 token），KV 缓存都能显著减少计算开销，提升推理效率。希望这个详细的解释能满足您的需求，如果还有任何疑问，欢迎进一步提问！

此处实验的模型head\_dims=128，因此由run\_mha\_fwd()->run\_mha\_fwd\_()->run\_mha\_fwd\_hdim128()

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/54b0bacc-cb5e-452f-af1c-0cde98e57ea8.webp)

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/6b529c01-7899-43ef-aae5-94948411b102.webp)

run\_flash\_fwd函数如下

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/meonaA205jEVnXxj/img/9c0c761b-c6f1-48c3-aa2a-6677ee7653db.webp)