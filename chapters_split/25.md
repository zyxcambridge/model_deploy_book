# 七.LLM端侧大模型部署


 解决了五个关键的 ODML 限制：延迟（没有到服务器的往返）、隐私（没有个人数据离开设备）、连接性（不需要互联网连接）、大小（简化模型）和二进制大小）和功耗（高效推理和缺乏网络连接）。

[https://zhuanlan.zhihu.com/p/27181462601: https://zhuanlan.zhihu.com/p/27181462601](https://zhuanlan.zhihu.com/p/27181462601)

## 计算优化、通信加速和存储架构

这些优化手段，可以分为三类。分别是计算，通信，和存储。 一、计算：算力加速与硬件优化 FlashMLA：针对英伟达Hopper架构GPU优化的解码加速技术，通过动态内存调度和并行计算优化，显著提升大模型推理速度，在H800 GPU上实现3000GB/s内存带宽和580TFLOPS计算性能，支持变长序列处理，显存占用降低45%。 DeepGEMM：基于FP8（8位浮点）的通用矩阵乘法库，支持混合专家模型（MoE）训练与推理加速，在Hopper GPU上实现1350 TFLOPs计算性能，代码仅300行，小批量场景加速比达2.7倍结合即时编译（JIT）与CUDA核心优化，平衡速度与精度，减少显存占用。 二、通信：智算集群通信优化与并行计算 DeepEP：首个面向MoE模型的开源专家并行通信库，支持低精度（如FP8）数据传输，优化多GPU间通信效率。 DualPipe（双向流水线并行算法）解决传统流水线并行中的等待时间问题，通过双向调度实现计算与通信重叠，提升训练效率。 EPLB（专家并行负载均衡器） 动态分配专家模型任务至空闲GPU，避免资源闲置。支持冗余专家与动态调整高负载专家。 三、存储：智算集群存储与数据处理 3FS（高性能分布式文件系统）解决AI训练中TB/PB级数据的存储与访问瓶颈，支持SSD+RDMA硬件优化，实现超高吞吐（6.6 TiB/s）与低延迟。解耦计算与存储资源，支持强一致性，适用于模型检查点保存、向量搜索等场景。 Smallpond基于3FS的数据处理框架，简化数据清洗、转换与加载（ETL），支持大规模AI训练与实时分析。

## 大模型应用开发中如何降低 时间延迟，提高程序的反应速度到至少和人一样的速度

你希望我用具体的例子进一步解释**预填充阶段（Prefill）和解码阶段（Decode）**，并且探讨它们的计算瓶颈（计算带宽与内存带宽）以及时间复杂度是如何影响实际延迟的。下面我会通过具体的例子，详细解释这两个阶段的工作原理、瓶颈以及它们的计算复杂度。

### 1. 预填充阶段（Prefill）示例

假设我们有一个输入句子：“今天天气怎么样”，它分成 10 个 token（例如：“今天”，“天气”，“怎么样” 等）。我们使用一个具有 32 层的 Transformer 模型，预填充阶段会为这个输入计算所有 token 的 Key 和 Value（KV）缓存，为后续的解码阶段做准备。

*   **步骤**：在预填充阶段，模型需要一次性计算整个输入序列的 KV 缓存。由于注意力机制的自注意力性质，每个 token 都需要与其他所有 token 进行交互。因此，对于每个 token，它与其他所有 token 之间的关系需要计算并存储下来。
    
*   **计算量**：
    
    *   假设序列长度是 10，那么每一层需要计算一个 \(10 \times 10\) 的矩阵，每个矩阵中包含了每个 token 和其他 token 的关系。每一层的计算量为 \(10 \times 10 = 100\) 次操作。
        
    *   模型有 32 层，意味着总的计算量是 \(32 \times 100 = 3200\) 次操作。
        
*   **瓶颈：计算带宽**：
    
    *   这个阶段的计算密集度较高，主要瓶颈在于 GPU 的计算能力。每一层都需要执行大量的矩阵乘法操作，这需要大量的浮点运算（FLOPS）。如果 GPU 计算能力不足，预填充的时间就会增加。
        
*   **时间复杂度**：
    
    *   每一层的注意力计算时间复杂度为 \(O(n^2)\)，因为每个 token 与其他 token 的关系都要计算。若模型有 \(L\) 层，总的时间复杂度为 \(O(L \cdot n^2)\)，其中 \(n\) 是序列的长度（这里是 10），\(L\) 是层数（这里是 32）。
        

### 2. 解码阶段（Decode）示例

假设输入的句子已经经过预填充，现在进入解码阶段。模型开始根据输入逐个生成输出 token。假设我们要生成的输出是：“很好”。

*   **步骤**：每次解码时，模型根据已经生成的 token 以及输入序列中的 KV 缓存，生成下一个 token。例如，生成第一个 token 时，模型加载输入的 KV 缓存，然后计算当前 token 的注意力；生成第二个 token 时，模型加载新的 KV 缓存，这个缓存包含了第一个生成的 token。
    
*   **内存访问**：
    
    *   解码过程需要加载历史 token 的 KV 缓存。假设输入有 10 个 token，生成第一个输出 token 时，模型只需要加载这 10 个 token 的 KV 缓存；生成第二个输出 token 时，模型需要加载 11 个 token 的 KV 缓存（包括第一个生成的 token），以此类推。
        
*   **瓶颈：内存带宽**：
    
    *   每次解码时，虽然计算量相对较小（只需要计算当前 token 与历史 token 的关系），但需要频繁从内存中读取大块的 KV 缓存。内存带宽成为瓶颈，如果 GPU 的内存带宽不足，会导致解码速度变慢。
        
    *   举个例子，如果每层 KV 缓存占用 1MB，序列长度为 1000，模型有 32 层，那么总的 KV 缓存大小大约是 32GB。GPU 需要快速读取这些数据，而内存带宽通常低于计算带宽（比如 100 TFLOPS 的计算能力和 3 TB/s 的内存带宽），这就可能会导致解码阶段的瓶颈。
        
*   **时间复杂度**：
    
    *   每生成一个新的 token，计算的时间复杂度是 \(O(n)\)，因为它只需要关注当前 token 和所有历史 token 的关系。如果生成 \(m\) 个 token，总的时间复杂度为 \(O(m \cdot n)\)，其中 \(m\) 是生成的 token 数量。
        

### 3. 预填充与解码的对比

| **阶段** | **任务** | **瓶颈** | **时间复杂度** | **依赖资源** |
| --- | --- | --- | --- | --- |
| **预填充** | 计算所有 token 的 KV | 计算带宽 | \( O(L \cdot n^2) \) | GPU 计算能力 |
| **解码** | 逐个生成 token | 内存带宽 | \( O(m \cdot n) \) | GPU 内存带宽 |

### 4. 总结：为什么预填充与解码的瓶颈不同？

*   **预填充阶段**：需要进行大量的矩阵乘法，涉及全序列的计算，计算量大，随着序列长度的增加计算复杂度会急剧增加。这个阶段的瓶颈主要是计算带宽，即 GPU 计算能力，决定了预填充所需的时间。
    
*   **解码阶段**：每次生成一个 token 时，计算量相对较小，只需要关注当前 token 和历史 token 之间的关系。但是，由于需要频繁从内存加载 KV 缓存，内存带宽成为主要的瓶颈。如果内存带宽不足，解码的延迟就会显著增加。
    

通过这些例子，你可以看到预填充和解码阶段的瓶颈和计算复杂度是如何影响实际延迟的。预填充依赖于计算能力，而解码则受限于内存带宽。这两个阶段的瓶颈不同，优化时需要分别关注不同的硬件资源。

如果你有更多问题或想要更深入地探讨某个方面，随时告诉我！

## 移动端部署 LLM

andriod Gemini Nano 

AI core

[https://developer.android.com/ai/gemini-nano/experimental?hl=zh-CN: https://developer.android.com/ai/gemini-nano/experimental?hl=zh-CN](https://developer.android.com/ai/gemini-nano/experimental?hl=zh-CN)

[https://developer.android.google.cn/ai/gemini-nano: https://developer.android.google.cn/ai/gemini-nano](https://developer.android.google.cn/ai/gemini-nano)